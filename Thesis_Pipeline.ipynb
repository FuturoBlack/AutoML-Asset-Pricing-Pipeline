{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6XWRVZOW0WK"
      },
      "outputs": [],
      "source": [
        "#==============================================================================\n",
        "# Copyright (C) 2025 Mohammad Rasoul Mostafavi Marian\n",
        "#\n",
        "# This program is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "#\n",
        "# This program is distributed in the hope that it will be useful,\n",
        "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "# GNU General Public License for more details.\n",
        "#\n",
        "# You should have received a copy of the GNU General Public License\n",
        "# along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
        "#==============================================================================\n",
        "\n",
        "#==============================================================================\n",
        "#                  --- MOSTAFAVI'S THESIS PIPELINE SCRIPT ---\n",
        "#\n",
        "# Author: Mohammad Rasoul Mostafavi Marian\n",
        "# Email: m.rasoulmostafavi@gmail.com\n",
        "# Last Edited: 2025 October 15\n",
        "# Version: 1.0\n",
        "#==============================================================================\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 1: INITIALIZE ENVIRONMENT\n",
        "#==============================================================================\n",
        "print(\"===========================================================================================\")\n",
        "print(\"STEP 1: Installing and Importing Required Libraries...\")\n",
        "print(\"===========================================================================================\")\n",
        "try:\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    #--> Sub-step 1.1: Installing Required Libraries:\n",
        "    print(\"--> Sub-step 1.1: Installing Required Libraries...\")\n",
        "    software_to_install = sorted([\n",
        "        'h2o','java','matplotlib','numpy','pandas','pandas-datareader','polars',\n",
        "        'psutil','pyarrow','scikit-learn','scipy','shap','statsmodels','ta','yfinance'\n",
        "    ])\n",
        "    installation_failed = False\n",
        "    for software in software_to_install:\n",
        "        try:\n",
        "            if software == 'java':\n",
        "                try:\n",
        "                    subprocess.check_call(['java', '-version'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "                    print(f\"[SUCCESS] Installed '{software}' (Pre-installed)\")\n",
        "                except:\n",
        "                    subprocess.check_call(['apt-get', 'update'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "                    subprocess.check_call(['apt-get', 'install', '-y', 'default-jre'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "                    print(f\"[SUCCESS] Installed '{software}'\")\n",
        "            else:\n",
        "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", software, \"-q\"])\n",
        "                print(f\"[SUCCESS] Installed '{software}'\")\n",
        "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
        "            print(f\"[ERROR] Failed to Install '{software}'\")\n",
        "            installation_failed = True\n",
        "    if installation_failed:\n",
        "        raise Exception(\"One or More Packages Failed to Install.\")\n",
        "    print(\"[SUCCESS] Sub-step 1.1: Installing Required Libraries Completed Successfully.\")\n",
        "\n",
        "    #--> Sub-step 1.2: Importing Required Libraries:\n",
        "    print(\"\\n--> Sub-step 1.2: Importing Required Libraries...\")\n",
        "    libraries_to_import = {\n",
        "        'google.colab': 'from google.colab import files',\n",
        "        'h2o': 'import h2o',\n",
        "        'h2o.automl': 'from h2o.automl import H2OAutoML',\n",
        "        'io': 'import io',\n",
        "        'logging': 'import logging',\n",
        "        'matplotlib.colors': 'import matplotlib.colors as mcolors',\n",
        "        'matplotlib.pyplot': 'import matplotlib.pyplot as plt',\n",
        "        'numpy': 'import numpy as np',\n",
        "        'os': 'import os',\n",
        "        'pandas': 'import pandas as pd',\n",
        "        'pandas_datareader': 'from pandas_datareader import data as pdr',\n",
        "        'psutil': 'import psutil',\n",
        "        'scipy.stats': 'from scipy.stats import norm',\n",
        "        'shap': 'import shap',\n",
        "        'sklearn': 'import sklearn',\n",
        "        'sklearn.linear_model': 'from sklearn.linear_model import LinearRegression',\n",
        "        'sklearn.metrics': 'from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score',\n",
        "        'sklearn.preprocessing': 'from sklearn.preprocessing import StandardScaler',\n",
        "        'statsmodels.tsa.stattools': 'from statsmodels.tsa.stattools import adfuller',\n",
        "        'ta': 'import ta',\n",
        "        'time': 'import time',\n",
        "        'yfinance': 'import yfinance as yf'\n",
        "    }\n",
        "    import_failed = False\n",
        "    for name, statement in sorted(libraries_to_import.items()):\n",
        "        try:\n",
        "            exec(statement, globals())\n",
        "            print(f\"[SUCCESS] Imported '{name}'\")\n",
        "        except ImportError as e:\n",
        "            if name == 'google.colab':\n",
        "                print(f\"[INFO] Skipped Importing '{name}' (Likely Not in a Colab Environment).\")\n",
        "            else:\n",
        "                print(f\"[ERROR] Failed to Import '{name}'. Reason: {e}\")\n",
        "                import_failed = True\n",
        "    if import_failed:\n",
        "        raise Exception(\"One or More Libraries Failed to Import.\")\n",
        "    print(\"[SUCCESS] Sub-step 1.2: Importing Required Libraries Completed Successfully.\")\n",
        "\n",
        "    IS_ENVIRONMENT_READY = True\n",
        "    print(\"\\n[SUCCESS] Step 1: Installing and Importing Required Libraries Completed Successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n[ERROR] Library Setup Failed: {e}\")\n",
        "    IS_ENVIRONMENT_READY = False\n",
        "finally:\n",
        "    print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 2: VERIFY LOCAL DATA ASSETS\n",
        "#==============================================================================\n",
        "print(\"\\n===========================================================================================\")\n",
        "print(\"STEP 2: Defining Required Internal Data...\")\n",
        "print(\"===========================================================================================\")\n",
        "try:\n",
        "    print(\"Defining Internal Company Data...\")\n",
        "    TICKER_INFO_CSV = \"\"\"Symbol,   StartDate,   EndDate,   Company\n",
        "AA,     1995-09-09,   2013-09-23,   Alcoa\n",
        "AAPL,   2015-03-19,   2025-09-28,   \"Apple Inc.\"\n",
        "AIG,    2004-04-08,   2008-09-22,   American International Group\n",
        "ALD,    1995-09-09,   2020-08-31,   AlliedSignal\n",
        "AMGN,   2020-08-31,   2025-09-28,   Amgen\n",
        "AMZN,   2024-02-26,   2025-09-28,   Amazon\n",
        "AXP,    1995-09-09,   2025-09-28,   American Express\n",
        "BA,     1995-09-09,   2025-09-28,   Boeing\n",
        "BAC,    2008-02-19,   2013-09-23,   Bank of America\n",
        "BS,     1995-09-09,   1997-03-17,   Bethlehem Steel\n",
        "CAT,    1995-09-09,   2025-09-28,   Caterpillar\n",
        "CRM,    2020-08-31,   2025-09-28,   Salesforce\n",
        "CSCO,   2009-06-08,   2025-09-28,   Cisco Systems\n",
        "CVX,    1995-09-09,   1999-11-01,   Chevron\n",
        "CVX,    2008-02-19,   2025-09-28,   Chevron\n",
        "DD,     1995-09-09,   2017-09-01,   DuPont\n",
        "DIS,    1995-09-09,   2025-09-28,   Disney\n",
        "DOW,    2019-04-02,   2024-11-08,   \"Dow Inc.\"\n",
        "DWDP,   2017-09-01,   2019-04-02,   DowDuPont\n",
        "GE,     1995-09-09,   2018-06-26,   General Electric\n",
        "GS,     2013-09-23,   2025-09-28,   \"Goldman Sachs Group, Inc.\"\n",
        "GT,     1995-09-09,   1999-11-01,   Goodyear Tire\n",
        "HD,     1999-11-01,   2025-09-28,   Home Depot\n",
        "HON,    2020-08-31,   2025-09-28,   Honeywell\n",
        "HWP,    1997-03-17,   2013-09-23,   Hewlett-Packard\n",
        "IBM,    1995-09-09,   2025-09-28,   IBM\n",
        "INTC,   1999-11-01,   2024-11-08,   Intel\n",
        "IP,     1995-09-09,   2004-04-08,   International Paper\n",
        "JNJ,    1997-03-17,   2025-09-28,   Johnson & Johnson\n",
        "JPM,    1995-09-09,   2025-09-28,   JPMorgan Chase\n",
        "KFT,    2008-09-22,   2012-09-24,   \"Kraft Foods Inc.\"\n",
        "KO,     1995-09-09,   2025-09-28,   Coca-Cola\n",
        "MCD,    1995-09-09,   2025-09-28,   McDonald's\n",
        "MMM,    1995-09-09,   2025-09-28,   3M\n",
        "MRK,    1995-09-09,   2025-09-28,   Merck\n",
        "MSFT,   1999-11-01,   2025-09-28,   Microsoft\n",
        "NKE,    2013-09-23,   2025-09-28,   \"Nike, Inc.\"\n",
        "NVDA,   2024-11-08,   2025-09-28,   Nvidia\n",
        "PFE,    2004-04-08,   2020-08-31,   Pfizer\n",
        "PG,     1995-09-09,   2025-09-28,   Procter & Gamble\n",
        "RTX,    2020-04-06,   2020-08-31,   Raytheon Technologies\n",
        "S,      1995-09-09,   1999-11-01,   Sears Roebuck\n",
        "SBC,    1999-11-01,   2015-03-19,   SBC Communications\n",
        "SHW,    2024-11-08,   2025-09-28,   Sherwin-Williams\n",
        "T,      1995-09-09,   2015-03-19,   AT&T\n",
        "TRV,    1997-03-17,   2009-06-08,   \"Travelers Inc.\"\n",
        "TRV,    2009-06-08,   2025-09-28,   \"The Travelers Companies, Inc.\"\n",
        "TX,     1995-09-09,   1997-03-17,   Texaco\n",
        "UK,     1995-09-09,   1999-11-01,   Union Carbide\n",
        "UNH,    2012-09-24,   2025-09-28,   UnitedHealth Group\n",
        "UTX,    1995-09-09,   2020-04-06,   United Technologies\n",
        "V,      2013-09-23,   2025-09-28,   \"Visa Inc.\"\n",
        "VZ,     2004-04-08,   2025-09-28,   Verizon Communications\n",
        "WBA,    2018-06-26,   2024-02-26,   Walgreens Boots Alliance\n",
        "WMT,    1997-03-17,   2025-09-28,   Walmart\n",
        "WX,     1995-09-09,   1997-03-17,   Westinghouse Electric\n",
        "XON,    1995-09-09,   2020-08-31,   Exxon\n",
        "Z,      1995-09-09,   1997-03-17,   \"F. W. Woolworth Company\"\n",
        "\"\"\"\n",
        "    ticker_info_df = pd.read_csv(io.StringIO(TICKER_INFO_CSV), skipinitialspace=True)\n",
        "    print(\"[SUCCESS] Internal Company Data Defined.\")\n",
        "\n",
        "    print(\"Defining Company Name Map...\")\n",
        "    SYMBOL_TO_NAME_MAP = {\n",
        "        'AA':    'Alcoa',                     'AAPL':  'AppleInc.',           'AIG':   'AmericanInternationalGroup','ALD':   'AlliedSignal',\n",
        "        'AMGN':  'Amgen',                     'AMZN':  'Amazon',              'AXP':   'AmericanExpress',           'BA':    'Boeing',\n",
        "        'BAC':   'BankofAmerica',             'BS':    'BethlehemSteel',      'CAT':   'Caterpillar',               'CRM':   'Salesforce',\n",
        "        'CSCO':  'CiscoSystems',              'CVX':   'Chevron',             'DD':    'DuPont',                    'DIS':   'Disney',\n",
        "        'DOW':   'DowInc.',                   'DWDP':  'DowDuPont',           'GE':    'GeneralElectric',           'GS':    'GoldmanSachsGroup,Inc.',\n",
        "        'GT':    'GoodyearTire',              'HD':    'HomeDepot',           'HON':   'Honeywell',                 'HWP':   'HewlettPackard',\n",
        "        'IBM':   'IBM',                       'INTC':  'Intel',               'IP':    'InternationalPaper',        'JNJ':   'JohnsonJohnson',\n",
        "        'JPM':   'JPMorganChase',             'KFT':   'KraftFoodsInc.',      'KO':    'CocaCola',                  'MCD':   'McDonalds',\n",
        "        'MMM':   '3M',                        'MRK':   'Merck',               'MSFT':  'Microsoft',                 'NKE':   'Nike,Inc.',\n",
        "        'NVDA':  'Nvidia',                    'PFE':   'Pfizer',              'PG':    'ProcterGamble',             'RTX':   'RaytheonTechnologies',\n",
        "        'S':     'SearsRoebuck',              'SBC':   'SBCCommunications',   'SHW':   'SherwinWilliams',           'T':     'ATT',\n",
        "        'TRV':   'TheTravelersCompanies,Inc.','TX':    'Texaco',              'UK':    'UnionCarbide',              'UNH':   'UnitedHealthGroup',\n",
        "        'UTX':   'UnitedTechnologies',        'V':     'VisaInc.',            'VZ':    'VerizonCommunications',     'WBA':   'WalgreensBootsAlliance',\n",
        "        'WMT':   'Walmart',                   'WX':    'WestinghouseElectric','XON':   'Exxon',                     'Z':     'FWWoolworthCompany',\n",
        "    }\n",
        "    print(\"[SUCCESS] Company Name Map Defined.\")\n",
        "\n",
        "    print(\"Defining FRED Macroeconomic Data Map...\")\n",
        "    FRED_TICKERS_MAP = {\n",
        "        '_CPIAUCSL':  'CPIAUCSL',   '_FEDFUNDS':  'FEDFUNDS',   '_GDPC1':    'GDPC1',     '_INDPRO':  'INDPRO',\n",
        "        '_PPIACO':    'PPIACO',     '_RSAFS':     'RSAFS',      '_UMCSENT':  'UMCSENT',   '_UNRATE':  'UNRATE',\n",
        "    }\n",
        "    print(\"[SUCCESS] FRED Macroeconomic Data Map Defined.\")\n",
        "\n",
        "    print(\"Defining Global Market Data Map...\")\n",
        "    YFINANCE_TICKERS_MAP = {\n",
        "        '_Daily_Close_VIX':             '^VIX',    '_Daily_Log_Return_DAX40':        '^GDAXI',   '_Daily_Log_Return_DXY':     'DX-Y.NYB',\n",
        "        '_Daily_Log_Return_FTSE100':    '^FTSE',   '_Daily_Log_Return_Gold':         'GC=F',     '_Daily_Log_Return_NASDAQ':  '^IXIC',\n",
        "        '_Daily_Log_Return_Nikkei225':  '^N225',   '_Daily_Log_Return_Russell2000':  '^RUT',     '_Daily_Log_Return_SP500':   '^GSPC',\n",
        "        '_Daily_Log_Return_WTI':        'CL=F',\n",
        "    }\n",
        "    print(\"[SUCCESS] Global Market Data Map Defined.\")\n",
        "\n",
        "    print(\"Defining Internal Event Data...\")\n",
        "    EVENTS_DATA_CSV = \"\"\"Variable,   StartDate,   EndDate,   Event\n",
        "_EO,    1996-11-05,   1996-11-05,   US Presidential Election 1996\n",
        "_EO,    2000-11-07,   2000-11-07,   US Presidential Election 2000\n",
        "_EO,    2004-11-02,   2004-11-02,   US Presidential Election 2004\n",
        "_EO,    2008-11-04,   2008-11-04,   US Presidential Election 2008\n",
        "_EO,    2012-11-06,   2012-11-06,   US Presidential Election 2012\n",
        "_EO,    2016-11-08,   2016-11-08,   US Presidential Election 2016\n",
        "_EO,    2020-11-03,   2020-11-03,   US Presidential Election 2020\n",
        "_EO,    2024-11-05,   2024-11-05,   US Presidential Election 2024\n",
        "_ND,    1995-07-12,   1995-07-16,   Chicago Heat Wave\n",
        "_ND,    1995-10-04,   1995-10-04,   Hurricane Opal\n",
        "_ND,    1996-01-06,   1996-01-12,   North American Blizzard of 1996\n",
        "_ND,    1997-04-17,   1997-05-01,   Red River Flood\n",
        "_ND,    1998-09-21,   1998-09-29,   Hurricane Georges\n",
        "_ND,    1999-05-03,   1999-05-03,   Bridge Creek-Moore Tornado Outbreak\n",
        "_ND,    1999-09-14,   1999-09-16,   Hurricane Floyd\n",
        "_ND,    2002-01-01,   2003-12-31,   Widespread Drought Period\n",
        "_ND,    2003-08-14,   2003-08-15,   Northeast Blackout of 2003\n",
        "_ND,    2004-08-13,   2004-08-14,   Hurricane Charley\n",
        "_ND,    2004-09-04,   2004-09-24,   Hurricane Ivan\n",
        "_ND,    2005-08-23,   2005-08-31,   Hurricane Katrina\n",
        "_ND,    2005-09-20,   2005-09-24,   Hurricane Rita\n",
        "_ND,    2005-10-18,   2005-10-24,   Hurricane Wilma\n",
        "_ND,    2008-09-01,   2008-09-15,   Hurricane Ike\n",
        "_ND,    2010-02-05,   2010-02-06,   North American Blizzard (Snowmageddon)\n",
        "_ND,    2011-04-25,   2011-04-28,   Super Outbreak of Tornadoes\n",
        "_ND,    2011-05-21,   2011-05-26,   Joplin Tornado Outbreak\n",
        "_ND,    2011-08-22,   2011-08-29,   Hurricane Irene\n",
        "_ND,    2012-01-01,   2012-12-31,   Widespread Drought Period\n",
        "_ND,    2012-10-22,   2012-11-02,   Hurricane Sandy\n",
        "_ND,    2013-09-11,   2013-09-15,   Colorado Floods\n",
        "_ND,    2016-08-12,   2016-08-16,   Louisiana Floods\n",
        "_ND,    2017-08-17,   2017-09-02,   Hurricane Harvey\n",
        "_ND,    2017-09-06,   2017-09-12,   Hurricane Irma\n",
        "_ND,    2017-09-16,   2017-10-02,   Hurricane Maria\n",
        "_ND,    2017-10-08,   2017-10-31,   Northern California Wildfires\n",
        "_ND,    2018-09-11,   2018-09-19,   Hurricane Florence\n",
        "_ND,    2018-10-10,   2018-10-11,   Hurricane Michael\n",
        "_ND,    2018-11-08,   2018-11-25,   Camp Fire (California)\n",
        "_ND,    2019-03-13,   2019-04-01,   Midwestern US Floods\n",
        "_ND,    2019-08-28,   2019-09-06,   Hurricane Dorian\n",
        "_ND,    2020-08-10,   2020-08-10,   Midwest Derecho\n",
        "_ND,    2020-08-16,   2020-12-31,   August Complex Fire (California)\n",
        "_ND,    2020-08-22,   2020-08-29,   Hurricane Laura\n",
        "_ND,    2021-02-13,   2021-02-17,   Texas Power Crisis (Major Winter Storm)\n",
        "_ND,    2021-06-26,   2021-07-23,   Western North American Heat Wave\n",
        "_ND,    2021-08-29,   2021-09-01,   Hurricane Ida\n",
        "_ND,    2021-12-10,   2021-12-11,   Quad-State Tornado Outbreak\n",
        "_ND,    2022-09-23,   2022-10-02,   Hurricane Ian\n",
        "_ND,    2022-12-21,   2022-12-26,   North American winter storm (Elliott)\n",
        "_PSI,   1995-04-19,   1995-04-19,   Oklahoma City Bombing\n",
        "_PSI,   1995-11-14,   1995-11-19,   US Government Shutdown\n",
        "_PSI,   1995-12-16,   1996-01-06,   US Government Shutdown\n",
        "_PSI,   1998-08-17,   1998-10-15,   Russian Financial Crisis & LTCM Collapse\n",
        "_PSI,   1998-12-19,   1999-02-12,   Impeachment of President Bill Clinton\n",
        "_PSI,   1999-07-01,   1999-12-31,   Y2K Scare Peak Period\n",
        "_PSI,   2000-03-10,   2002-10-09,   Dot-com Bubble Burst Period\n",
        "_PSI,   2001-09-11,   2001-09-11,   September 11th Terrorist Attacks\n",
        "_PSI,   2001-10-07,   2001-10-07,   Start of War in Afghanistan\n",
        "_PSI,   2001-12-02,   2002-07-21,   Enron & WorldCom Corporate Scandals\n",
        "_PSI,   2003-03-19,   2003-03-19,   Start of Iraq War\n",
        "_PSI,   2008-09-15,   2009-06-30,   Global Financial Crisis Peak Instability\n",
        "_PSI,   2010-04-27,   2012-12-31,   European Sovereign Debt Crisis Impact Period\n",
        "_PSI,   2011-01-14,   2011-10-23,   Arab Spring Peak Impact\n",
        "_PSI,   2011-07-01,   2011-08-08,   US Debt Ceiling Crisis & Credit Rating Downgrade\n",
        "_PSI,   2013-05-22,   2013-08-31,   Fed Taper Tantrum\n",
        "_PSI,   2013-10-01,   2013-10-17,   US Government Shutdown\n",
        "_PSI,   2014-08-09,   2014-11-30,   Ferguson Unrest Period\n",
        "_PSI,   2016-06-23,   2016-06-23,   Brexit Referendum\n",
        "_PSI,   2018-01-20,   2018-01-22,   US Government Shutdown\n",
        "_PSI,   2018-02-09,   2018-02-09,   US Government Shutdown (Short)\n",
        "_PSI,   2018-05-08,   2018-05-08,   US Withdrawal from Iran Nuclear Deal\n",
        "_PSI,   2018-12-22,   2019-01-25,   US Government Shutdown (Longest)\n",
        "_PSI,   2019-12-18,   2020-02-05,   First Impeachment of President Donald Trump\n",
        "_PSI,   2020-02-20,   2020-04-07,   COVID-19 Stock Market Crash\n",
        "_PSI,   2020-03-11,   2020-03-13,   WHO Declares Pandemic & US Declares National Emergency\n",
        "_PSI,   2020-05-25,   2020-08-31,   George Floyd Protests Peak Period\n",
        "_PSI,   2021-01-06,   2021-01-06,   January 6th Capitol Attack\n",
        "_PSI,   2021-01-13,   2021-02-13,   Second Impeachment of President Donald Trump\n",
        "_PSI,   2021-08-15,   2021-08-31,   Fall of Kabul & Afghanistan Withdrawal\n",
        "_PSI,   2022-02-24,   2022-02-24,   Start of Russia-Ukraine Invasion\n",
        "_PSI,   2023-03-10,   2023-05-01,   US Regional Banking Crisis (SVB Collapse)\n",
        "_PSI,   2023-05-15,   2023-06-03,   US Debt Ceiling Crisis 2023\n",
        "_TWI,   2018-03-22,   2020-01-15,   Main Period of US-China Trade War and Tariffs\n",
        "\"\"\"\n",
        "    print(\"[SUCCESS] Internal Event Data Defined.\")\n",
        "\n",
        "    print(\"Defining Color Palette...\")\n",
        "    COLOR_PALETTE = {\n",
        "        'Color1': '#FFFFFF',   # White\n",
        "        'Color2': '#000000',   # Black\n",
        "        'Color3': '#E7E6E6',   # Light Gray\n",
        "        'Color4': '#4472C4',   # Blue\n",
        "        'Color5': '#70AD47',   # Green\n",
        "        'Color6': '#5B9BD5',   # Light Blue\n",
        "        'Color7': '#92D050',   # Light Green\n",
        "        'Color8': '#264478',   # Dark Blue\n",
        "        'Color9': '#006400',   # Dark Green\n",
        "    }\n",
        "    print(\"[SUCCESS] Color Palette Defined.\")\n",
        "\n",
        "    ARE_DATA_ASSETS_READY = True\n",
        "    print(\"\\n[SUCCESS] Step 2: Defining Required Internal Data Completed Successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n[ERROR] Internal Data Setup Failed: {e}\")\n",
        "    ARE_DATA_ASSETS_READY = False\n",
        "finally:\n",
        "    print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 3: CREATE BASE TIME-INDEXED DATAFRAME\n",
        "#==============================================================================\n",
        "def create_base_dataframe(start_date, end_date):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 3: Creating Base DataFrame with Time Features...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    try:\n",
        "        #--> Sub-step 3.1: Creating Base DataFrame:\n",
        "        print(\"--> Sub-step 3.1: Creating Base DataFrame...\")\n",
        "        date_index = pd.to_datetime(pd.date_range(start=start_date, end=end_date, freq='D'))\n",
        "        df = pd.DataFrame(index=date_index)\n",
        "        df['_Date'] = df.index\n",
        "        print(f\"[SUCCESS] Base DataFrame with '_Date' Column Created for the Range {start_date} to {end_date}.\")\n",
        "        print(\"[SUCCESS] Sub-step 3.1: Creating Base DataFrame Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 3.2: Generating Time Features:\n",
        "        print(\"\\n--> Sub-step 3.2: Generating Time Features...\")\n",
        "        features_to_create = {\n",
        "            '_Year': lambda d: d.index.year,\n",
        "            '_Month': lambda d: d.index.month,\n",
        "            '_Day': lambda d: d.index.day,\n",
        "            '_WeekOfYear': lambda d: d.index.isocalendar().week.astype(int),\n",
        "            '_DayOfWeek': lambda d: d.index.dayofweek\n",
        "        }\n",
        "        for name, func in features_to_create.items():\n",
        "            df[name] = func(df)\n",
        "            print(f\"[SUCCESS] Time Feature Column '{name}' Created.\")\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "        print(\"[SUCCESS] Sub-step 3.2: Generating Time Features Completed Successfully.\")\n",
        "\n",
        "        print(\"\\n[SUCCESS] Step 3: Creating Base DataFrame with Time Features Completed Successfully.\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] Failed to Create the Base DataFrame: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 4: BATCH DOWNLOAD CORPORATE OHLCV DATA - WARM-UP\n",
        "#==============================================================================\n",
        "def download_ohlcv_data_batch(ticker_info_df, start_date, end_date):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 4: Batch Downloading Corporate OHLCV Data Using Warm-Up...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    try:\n",
        "        symbol_to_company_map = pd.Series(ticker_info_df.Company.values, index=ticker_info_df.Symbol).to_dict()\n",
        "        unique_symbols = sorted(ticker_info_df['Symbol'].unique().tolist())\n",
        "        logging.getLogger('yfinance').setLevel(logging.CRITICAL)\n",
        "        warm_up_start_date = pd.to_datetime(start_date) - pd.Timedelta(days=110)\n",
        "        downloaded_data = yf.download(unique_symbols, start=warm_up_start_date, end=end_date, progress=False, auto_adjust=True)\n",
        "        logging.getLogger('yfinance').setLevel(logging.WARNING)\n",
        "        success_count, incomplete_count, failure_count = 0, 0, 0\n",
        "        for symbol in unique_symbols:\n",
        "            company_name = symbol_to_company_map.get(symbol, \"Unknown Company\")\n",
        "            if isinstance(downloaded_data.columns, pd.MultiIndex) and symbol in downloaded_data['Close'].columns and not downloaded_data['Close'][symbol].isnull().all():\n",
        "                symbol_entries = ticker_info_df[ticker_info_df['Symbol'] == symbol]\n",
        "                requested_start, requested_end = pd.to_datetime(symbol_entries['StartDate'].min()), pd.to_datetime(symbol_entries['EndDate'].max())\n",
        "                actual_start, actual_end = downloaded_data['Close'][symbol].first_valid_index(), downloaded_data['Close'][symbol].last_valid_index()\n",
        "                if actual_start <= requested_start and actual_end >= requested_end:\n",
        "                    print(f\"[SUCCESS] Downloaded Corporate OHLCV Data for Symbol: {symbol} ({company_name}). Full Period Covered.\")\n",
        "                    success_count += 1\n",
        "                else:\n",
        "                    missing_at_start, missing_at_end = max(0, (actual_start - requested_start).days), max(0, (requested_end - actual_end).days)\n",
        "                    print(f\"[SUCCESS] Downloaded Incomplete Corporate OHLCV Data for Symbol: {symbol} ({company_name}). Missing {missing_at_start} Days at Start and {missing_at_end} Days at End.\")\n",
        "                    incomplete_count += 1\n",
        "            else:\n",
        "                print(f\"[ERROR] Failed to Download Any Corporate OHLCV Data for Symbol: {symbol} ({company_name}).\")\n",
        "                failure_count += 1\n",
        "\n",
        "        print(f\"\\n[SUCCESS] Step 4: Batch Downloading Corporate OHLCV Data Completed Successfully. (Summary: {success_count} Symbols Fully Downloaded, {incomplete_count} Symbols Incompletely Downloaded, {failure_count} Symbols Failed).\")\n",
        "        return downloaded_data\n",
        "    except Exception as e:\n",
        "        logging.getLogger('yfinance').setLevel(logging.WARNING)\n",
        "        print(f\"\\n[ERROR] A Critical Error Occurred During the Batch Download Process: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 5: ENGINEER AND INTEGRATE CORPORATE OHLCV FEATURES\n",
        "#==============================================================================\n",
        "def calculate_features_for_one_stock(df: pd.DataFrame):\n",
        "    try:\n",
        "        df = df.resample('D').asfreq()\n",
        "        price_cols = [c for c in df.columns if 'Volume' not in c]\n",
        "        df[price_cols] = df[price_cols].ffill()\n",
        "        if 'Volume' in df.columns:\n",
        "            df['Volume'] = df['Volume'].fillna(0)\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "        safe_open = df['Open'].clip(lower=1e-9)\n",
        "        safe_high = df['High'].clip(lower=1e-9)\n",
        "        safe_low = df['Low'].clip(lower=1e-9)\n",
        "        safe_close = df['Close'].clip(lower=1e-9)\n",
        "        safe_volume = df['Volume'].clip(lower=1e-9)\n",
        "        features['_Daily_Log_Open'] = np.log(safe_open)\n",
        "        features['_Daily_Log_High'] = np.log(safe_high)\n",
        "        features['_Daily_Log_Low'] = np.log(safe_low)\n",
        "        features['_Daily_Log_Close'] = np.log(safe_close)\n",
        "        features['_Daily_Log_Volume'] = np.log(safe_volume)\n",
        "        features['_Daily_Log_Return'] = np.log(safe_close / safe_close.shift(1))\n",
        "        features['_SMA_20'] = ta.trend.sma_indicator(df['Close'], 20)\n",
        "        features['_SMA_50'] = ta.trend.sma_indicator(df['Close'], 50)\n",
        "        features['_EMA_20'] = ta.trend.ema_indicator(df['Close'], 20)\n",
        "        features['_EMA_50'] = ta.trend.ema_indicator(df['Close'], 50)\n",
        "        features['_ADX_14'] = ta.trend.ADXIndicator(df['High'], df['Low'], df['Close'], 14).adx()\n",
        "        features['_RSI_14'] = ta.momentum.rsi(df['Close'], 14)\n",
        "        macd = ta.trend.MACD(df['Close'], 26, 12, 9)\n",
        "        features['_MACD_line'] = macd.macd()\n",
        "        features['_MACD_signal'] = macd.macd_signal()\n",
        "        features['_MACD_hist'] = macd.macd_diff()\n",
        "        bb = ta.volatility.BollingerBands(df['Close'], 20, 2)\n",
        "        features['_BB_upper'] = bb.bollinger_hband()\n",
        "        features['_BB_mid'] = bb.bollinger_mavg()\n",
        "        features['_BB_lower'] = bb.bollinger_lband()\n",
        "        features['_ATR_14'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'], 14)\n",
        "        features['_OBV'] = ta.volume.on_balance_volume(df['Close'], df['Volume'])\n",
        "        features['_MFI_14'] = ta.volume.money_flow_index(df['High'], df['Low'], df['Close'], df['Volume'], 14)\n",
        "        features['_HV_5'] = features['_Daily_Log_Return'].rolling(5).std()\n",
        "        features['_HV_21'] = features['_Daily_Log_Return'].rolling(21).std()\n",
        "        features['_HV_63'] = features['_Daily_Log_Return'].rolling(63).std()\n",
        "        return features\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def process_and_merge_stock_features(master_df, multi_level_df, ticker_info_df, name_map):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 5: Engineering and Merging Corporate OHLCV Features...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    try:\n",
        "        final_df = master_df.copy()\n",
        "        if multi_level_df is None: raise ValueError(\"Input Data for Corporate Features is Invalid: None Object Received.\")\n",
        "        symbols_to_process = sorted(multi_level_df.columns.get_level_values(1).unique())\n",
        "        success_count, failure_count, skipped_count = 0, 0, 0\n",
        "        for symbol in symbols_to_process:\n",
        "            company_name = name_map.get(symbol, symbol)\n",
        "            single_stock_df = multi_level_df.xs(symbol, axis=1, level=1)\n",
        "            single_stock_df_filtered = single_stock_df.dropna(subset=['Close'])\n",
        "            if single_stock_df_filtered.empty:\n",
        "                print(f\"[INFO] Skipped Symbol {symbol} ({company_name}) Due to No Valid Data in its Date Range.\")\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "            features_df = calculate_features_for_one_stock(single_stock_df_filtered)\n",
        "            if features_df is not None:\n",
        "                final_df = pd.merge(final_df, features_df.add_suffix(f\"_{company_name}\"), left_on='_Date', right_index=True, how='left')\n",
        "                print(f\"[SUCCESS] Engineered and Merged Corporate OHLCV Features for Symbol: {symbol} ({company_name}).\")\n",
        "                success_count += 1\n",
        "            else:\n",
        "                print(f\"[ERROR] Failed to Engineer Corporate OHLCV Features for Symbol: {symbol} ({company_name}).\")\n",
        "                failure_count += 1\n",
        "\n",
        "        print(f\"\\n[SUCCESS] Step 5: Engineering and Merging Corporate OHLCV Features Completed Successfully. (Summary: {success_count} Symbols Processed, {failure_count} Failed, {skipped_count} Skipped).\")\n",
        "        return final_df\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] A Critical Error Occurred During the Feature Engineering and Merging Process: {e}\")\n",
        "        return master_df\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 6: FETCH AND MERGE FRED MACROECONOMIC DATA - WARM-UP\n",
        "#==============================================================================\n",
        "def fetch_and_merge_fred_data(master_df, fred_tickers_map):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 6: Fetching and Merging FRED Macroeconomic Data Using Warm-Up...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    processed_df = master_df.copy()\n",
        "    try:\n",
        "        #--> Sub-step 6.1: Fetching Macroeconomic Data:\n",
        "        print(\"--> Sub-step 6.1: Fetching Macroeconomic Data...\")\n",
        "        warm_up_start_date = '1994-01-01'\n",
        "        end_date = processed_df['_Date'].max()\n",
        "        fred_data = pdr.DataReader(sorted(list(fred_tickers_map.values())), 'fred', warm_up_start_date, end_date)\n",
        "        if fred_data.empty: raise ValueError(\"Downloaded FRED Data is Empty.\")\n",
        "        fred_data.rename(columns={v: k for k, v in fred_tickers_map.items()}, inplace=True)\n",
        "        for key in fred_tickers_map.keys():\n",
        "            print(f\"[SUCCESS] Fetched Macroeconomic Variable '{key}'.\")\n",
        "        print(\"[SUCCESS] Sub-step 6.1: Fetching Macroeconomic Data Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 6.2: Merging Macroeconomic Data using merge_asof:\n",
        "        print(\"\\n--> Sub-step 6.2: Merging Macroeconomic Data...\")\n",
        "        fred_data.index = pd.to_datetime(fred_data.index)\n",
        "        fred_data = fred_data.sort_index()\n",
        "        temp_df = processed_df.sort_values('_Date')\n",
        "        merged_df = pd.merge_asof(temp_df, fred_data, left_on='_Date', right_index=True, direction='backward')\n",
        "        for key in sorted(fred_tickers_map.keys()):\n",
        "            print(f\"[SUCCESS] Merged Macroeconomic Variable '{key}'.\")\n",
        "        print(\"[SUCCESS] Sub-step 6.2: Merging Macroeconomic Data Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 6.3: Handling Missing Values in Macroeconomic Data:\n",
        "        print(\"\\n--> Sub-step 6.3: Handling Missing Values in Macroeconomic Data...\")\n",
        "        merged_df[list(fred_tickers_map.keys())] = merged_df[list(fred_tickers_map.keys())].ffill()\n",
        "        print(\"[SUCCESS] Missing Values for Macroeconomic Data Have Been Forward-Filled.\")\n",
        "        print(\"[SUCCESS] Sub-step 6.3: Handling Missing Values in Macroeconomic Data Completed Successfully.\")\n",
        "\n",
        "        print(\"\\n[SUCCESS] Step 6: Fetching and Merging FRED Macroeconomic Data Completed Successfully.\")\n",
        "        return merged_df\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] An Error Occurred During FRED Data Integration: {e}\")\n",
        "        return processed_df\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 7: FETCH AND MERGE GLOBAL MARKET DATA - WARM-UP\n",
        "#==============================================================================\n",
        "def fetch_and_merge_yfinance_indices(master_df, yfinance_tickers_map):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 7: Fetching and Merging Global Market Data Using Warm-Up...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    try:\n",
        "        df = master_df.copy()\n",
        "        warm_up_start_date = df['_Date'].min() - pd.Timedelta(days=10)\n",
        "        end_date = df['_Date'].max()\n",
        "        ordered_keys = sorted(list(yfinance_tickers_map.keys()))\n",
        "\n",
        "        #--> Sub-step 7.1: Fetching Global Market Data:\n",
        "        print(\"--> Sub-step 7.1: Fetching Global Market Data...\")\n",
        "        logging.getLogger('yfinance').setLevel(logging.CRITICAL)\n",
        "        downloaded_data = yf.download(list(yfinance_tickers_map.values()), start=warm_up_start_date, end=end_date, progress=False, auto_adjust=True)\n",
        "        logging.getLogger('yfinance').setLevel(logging.WARNING)\n",
        "        processed_data = pd.DataFrame()\n",
        "        fetch_success_count = 0\n",
        "        fetch_failure_list = []\n",
        "        for key in ordered_keys:\n",
        "            ticker = yfinance_tickers_map[key]\n",
        "            if downloaded_data.empty or not isinstance(downloaded_data.columns, pd.MultiIndex) or ticker not in downloaded_data['Close'].columns or downloaded_data['Close'][ticker].isnull().all():\n",
        "                print(f\"[ERROR] Failed to Fetch Global Market Data for Variable '{key}'.\")\n",
        "                fetch_failure_list.append(key)\n",
        "                continue\n",
        "            close_price = downloaded_data['Close'][ticker].clip(lower=1e-9)\n",
        "            if '_Daily_Log_Return' in key:\n",
        "                series = np.log(close_price / close_price.shift(1))\n",
        "                series = series.resample('D').asfreq().fillna(0)\n",
        "            elif '_Daily_Close' in key:\n",
        "                series = close_price\n",
        "                series = series.resample('D').ffill()\n",
        "            processed_data[key] = series\n",
        "            print(f\"[SUCCESS] Fetched Global Market Data for Variable '{key}'.\")\n",
        "            fetch_success_count += 1\n",
        "        if fetch_failure_list: raise Exception(f\"Failed to Fetch the Following Required Variables: {fetch_failure_list}\")\n",
        "        print(\"[SUCCESS] Sub-step 7.1: Fetching Global Market Data Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 7.2: Merging Global Market Data:\n",
        "        print(\"\\n--> Sub-step 7.2: Merging Global Market Data...\")\n",
        "        merged_df = pd.merge(df, processed_data, left_on='_Date', right_index=True, how='left')\n",
        "        for key in ordered_keys:\n",
        "            print(f\"[SUCCESS] Merged Global Market Data for Variable '{key}'.\")\n",
        "        print(\"[SUCCESS] Sub-step 7.2: Merging Global Market Data Completed Successfully.\")\n",
        "\n",
        "        print(f\"\\n[SUCCESS] Step 7: Fetching and Merging Global Market Data Completed Successfully.\")\n",
        "        return merged_df\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] A Critical Error Occurred During the Global Market Data Fetching and Merging Process: {e}\")\n",
        "        return master_df\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 8: FILL QUALITATIVE DATA\n",
        "#==============================================================================\n",
        "def add_qualitative_variables(master_df, events_csv_string):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 8: Filling Qualitative Data...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    try:\n",
        "        df_with_events = master_df.copy()\n",
        "        events_df = pd.read_csv(io.StringIO(events_csv_string), skipinitialspace=True)\n",
        "        events_df['StartDate'] = pd.to_datetime(events_df['StartDate'])\n",
        "        events_df['EndDate'] = pd.to_datetime(events_df['EndDate'])\n",
        "        variable_columns = ['_EO', '_ND', '_PSI', '_TWI']\n",
        "        success_count = 0\n",
        "        for col in variable_columns:\n",
        "            try:\n",
        "                df_with_events[col] = 0\n",
        "                col_events = events_df[events_df['Variable'] == col]\n",
        "                for _, row in col_events.iterrows():\n",
        "                    mask = (df_with_events['_Date'] >= row['StartDate']) & (df_with_events['_Date'] <= row['EndDate'])\n",
        "                    df_with_events.loc[mask, col] = 1\n",
        "                print(f\"[SUCCESS] Filled Qualitative Data for Variable '{col}'.\")\n",
        "                success_count += 1\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] Failed to Fill Qualitative Data for Variable '{col}'. Reason: {e}\")\n",
        "        if success_count != len(variable_columns):\n",
        "            raise Exception(\"One or More Qualitative Variables Failed to be Filled.\")\n",
        "\n",
        "        print(f\"\\n[SUCCESS] Step 8: Filling Qualitative Data Completed Successfully.\")\n",
        "        return df_with_events\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] A Critical Error Occurred During the Qualitative Data Filling Process: {e}\")\n",
        "        return master_df\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 9: FETCH AND MERGE DEPENDENT VARIABLE DATA - WARM-UP\n",
        "#==============================================================================\n",
        "def calculate_dependent_variable(master_df):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 9: Fetching and Merging Dependent Variable Data Using Warm-Up...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    dependent_variable_name = '_Daily_Log_Return_DJI'\n",
        "    final_df = master_df.copy()\n",
        "    try:\n",
        "        #--> Sub-step 9.1: Fetching Dependent Variable Data:\n",
        "        print(\"--> Sub-step 9.1: Fetching Dependent Variable Data...\")\n",
        "        warm_up_start_date = master_df['_Date'].min() - pd.Timedelta(days=10)\n",
        "        end_date = master_df['_Date'].max()\n",
        "        dji_data = yf.download('^DJI', start=warm_up_start_date, end=end_date, auto_adjust=True, progress=False)\n",
        "        if isinstance(dji_data.columns, pd.MultiIndex):\n",
        "            dji_data.columns = dji_data.columns.droplevel(1)\n",
        "        if dji_data.empty or 'Close' not in dji_data.columns or dji_data['Close'].isnull().all():\n",
        "            raise ValueError(\"Downloaded DJI Data is Empty, Missing 'Close' Column, or Has No Valid Close Prices.\")\n",
        "        safe_close = dji_data['Close'].clip(lower=1e-9)\n",
        "        log_return_series = np.log(safe_close / safe_close.shift(1))\n",
        "        log_return_series.name = dependent_variable_name\n",
        "        print(f\"[SUCCESS] Fetched Dependent Variable Data for '{dependent_variable_name}'.\")\n",
        "        print(\"[SUCCESS] Sub-step 9.1: Fetching Dependent Variable Data Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 9.2: Merging Dependent Variable Data:\n",
        "        print(\"\\n--> Sub-step 9.2: Merging Dependent Variable Data...\")\n",
        "        final_df = master_df.merge(log_return_series, left_on='_Date', right_index=True, how='left')\n",
        "        print(f\"[SUCCESS] Merged Dependent Variable Data into the Master DataFrame.\")\n",
        "        print(\"[SUCCESS] Sub-step 9.2: Merging Dependent Variable Data Completed Successfully.\")\n",
        "\n",
        "        print(f\"\\n[SUCCESS] Step 9: Fetching and Merging Dependent Variable Data Completed Successfully.\")\n",
        "        return final_df\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] A Critical Error Occurred During the Dependent Variable Data Fetching and Merging Process: {e}\")\n",
        "        final_df[dependent_variable_name] = np.nan\n",
        "        return final_df\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 10: PRE-PROCESS - HANDLE MISSING VALUES - FORWARD FILL\n",
        "#==============================================================================\n",
        "def handle_missing_values(df, ticker_info_df, name_map):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 10: Pre-processing - Handling Missing Values Using Forward Fill...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    processed_df = df.copy()\n",
        "    try:\n",
        "        #--> Sub-step 10.1: Identifying Initial Statistics for Missing Values:\n",
        "        print(\"--> Sub-step 10.1: Identifying Initial Statistics for Missing Values...\")\n",
        "        total_cells = processed_df.size\n",
        "        initial_missing_cells = processed_df.isnull().sum().sum()\n",
        "        print(f\"[SUCCESS] Identified {initial_missing_cells} Missing Cells out of {total_cells} Total Cells.\")\n",
        "        print(\"[SUCCESS] Sub-step 10.1: Identifying Initial Statistics Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 10.2: Applying Fill Rules for Missing Values:\n",
        "        print(\"\\n--> Sub-step 10.2: Applying Fill Rules for Missing Values...\")\n",
        "        print(\"Applying Out-of-Index Rule...\")\n",
        "        unique_symbols = ticker_info_df['Symbol'].unique()\n",
        "        for symbol in unique_symbols:\n",
        "            company_name_suffix = name_map.get(symbol, symbol)\n",
        "            company_cols = [col for col in processed_df.columns if col.endswith(f\"_{company_name_suffix}\")]\n",
        "            if not company_cols: continue\n",
        "            symbol_periods = ticker_info_df[ticker_info_df['Symbol'] == symbol]\n",
        "            active_mask = pd.Series(False, index=processed_df.index)\n",
        "            for _, row in symbol_periods.iterrows():\n",
        "                active_mask |= (processed_df['_Date'] >= pd.to_datetime(row['StartDate'])) & (processed_df['_Date'] <= pd.to_datetime(row['EndDate']))\n",
        "            processed_df.loc[~active_mask, company_cols] = 0\n",
        "        print(\"[SUCCESS] Out-of-Index Rule Applied.\")\n",
        "        print(\"Applying Forward-Fill Rule...\")\n",
        "        exclude_from_ffill = {col for col in processed_df.columns if 'Volume' in col or '_Daily_Log_Return' in col}\n",
        "        exclude_from_ffill.update(['_Date', '_Year', '_Month', '_Day', '_WeekOfYear', '_DayOfWeek', '_EO', '_ND', '_PSI', '_TWI'])\n",
        "        ffill_cols = [col for col in processed_df.columns if col not in exclude_from_ffill]\n",
        "        processed_df[ffill_cols] = processed_df[ffill_cols].ffill()\n",
        "        print(\"[SUCCESS] Forward-Fill Rule Applied.\")\n",
        "        print(\"Applying Zero-Fill Rule...\")\n",
        "        processed_df.fillna(0, inplace=True)\n",
        "        print(\"[SUCCESS] Zero-Fill Rule Applied.\")\n",
        "        print(\"[SUCCESS] Sub-step 10.2: Applying Fill Rules Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 10.3: Calculating and Reporting Final Statistics:\n",
        "        print(\"\\n--> Sub-step 10.3: Calculating and Reporting Final Statistics...\")\n",
        "        final_missing_cells = processed_df.isnull().sum().sum()\n",
        "        close_price_cols = [col for col in processed_df.columns if '_Daily_Log_Close' in col]\n",
        "        mean_price_after, std_dev_price_after = 0, 0\n",
        "        if close_price_cols:\n",
        "            non_zero_log_prices = processed_df[close_price_cols].values.flatten()\n",
        "            non_zero_log_prices = non_zero_log_prices[non_zero_log_prices != 0]\n",
        "            if non_zero_log_prices.size > 0:\n",
        "                actual_prices = np.exp(non_zero_log_prices)\n",
        "                mean_price_after, std_dev_price_after = actual_prices.mean(), actual_prices.std()\n",
        "        initial_missing_percentage = (initial_missing_cells / total_cells) * 100 if total_cells > 0 else 0\n",
        "        final_missing_percentage = (final_missing_cells / total_cells) * 100 if total_cells > 0 else 0\n",
        "        print(\"--- Missing Value Handling Statistics ---\")\n",
        "        print(f\"1. Initial Missing Cells: {initial_missing_cells} ({initial_missing_percentage:.2f}%)\")\n",
        "        print(f\"2. Final Missing Cells: {final_missing_cells} ({final_missing_percentage:.2f}%)\")\n",
        "        print(f\"3. Mean Price (Post-Handling): ${mean_price_after:.2f}\")\n",
        "        print(f\"4. Price Std Dev (Post-Handling): {std_dev_price_after:.2f}\")\n",
        "        print(\"[SUCCESS] Sub-step 10.3: Calculating and Reporting Final Statistics Completed Successfully.\")\n",
        "\n",
        "        print(\"\\n[SUCCESS] Step 10: Pre-processing - Handling Missing Values Completed Successfully.\")\n",
        "        return processed_df\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] An Error Occurred During Missing Value Handling: {e}\")\n",
        "        return df\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 11: PRE-PROCESS - HANDLE OUTLIERS - WINSORIZATION\n",
        "#==============================================================================\n",
        "def handle_outliers(df):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 11: Pre-processing - Handling Outliers Using Winsorization...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    outlier_handling_df = df.copy()\n",
        "    try:\n",
        "        #--> Sub-step 11.1: Identifying Return Variables for Outlier Detection:\n",
        "        print(\"--> Sub-step 11.1: Identifying Return Variables for Outlier Detection...\")\n",
        "        outlier_target_cols = [col for col in outlier_handling_df.columns if '_Daily_Log_Return' in col]\n",
        "        if not outlier_target_cols:\n",
        "            print(\"[INFO] No Return Variables Found for Outlier Handling. Skipping Step.\")\n",
        "            print(\"===========================================================================================\")\n",
        "            return outlier_handling_df\n",
        "        total_data_points = outlier_handling_df[outlier_target_cols].size\n",
        "        print(f\"[SUCCESS] Identified {len(outlier_target_cols)} Return Variables with {total_data_points} Total Data Points.\")\n",
        "        print(\"[SUCCESS] Sub-step 11.1: Identifying Return Variables Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 11.2: Applying Winsorization Rule to Return Variables:\n",
        "        print(\"\\n--> Sub-step 11.2: Applying Winsorization Rule to Return Variables...\")\n",
        "        total_adjusted_points = 0\n",
        "        for col in outlier_target_cols:\n",
        "            if (outlier_handling_df[col] == 0).all(): continue\n",
        "            lower_bound, upper_bound = outlier_handling_df[col].quantile(0.005), outlier_handling_df[col].quantile(0.995)\n",
        "            if lower_bound < upper_bound:\n",
        "                outliers_mask = (outlier_handling_df[col] < lower_bound) | (outlier_handling_df[col] > upper_bound)\n",
        "                total_adjusted_points += outliers_mask.sum()\n",
        "                outlier_handling_df[col] = outlier_handling_df[col].clip(lower_bound, upper_bound)\n",
        "                print(f\"[SUCCESS] Applied Winsorization to Column '{col}'.\")\n",
        "        print(\"[SUCCESS] Sub-step 11.2: Applying Winsorization Rule Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 11.3: Calculating and Reporting Final Statistics:\n",
        "        print(\"\\n--> Sub-step 11.3: Calculating and Reporting Final Statistics...\")\n",
        "        adjustment_percentage = (total_adjusted_points / total_data_points) * 100 if total_data_points > 0 else 0\n",
        "        print(\"--- Outlier Handling Statistics ---\")\n",
        "        print(f\"1. Data Points Processed: {total_data_points}\")\n",
        "        print(f\"2. Data Points Adjusted: {total_adjusted_points}\")\n",
        "        print(f\"3. Adjustment Percentage: {adjustment_percentage:.4f}%\")\n",
        "        print(\"[SUCCESS] Sub-step 11.3: Calculating and Reporting Final Statistics Completed Successfully.\")\n",
        "\n",
        "        print(\"\\n[SUCCESS] Step 11: Pre-processing - Handling Outliers Completed Successfully.\")\n",
        "        return outlier_handling_df\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] An Error Occurred During Outlier Handling: {e}\")\n",
        "        return df\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 12: PERFORM STATIONARITY ANALYSIS - AUGMENTED DICKEY-FULLER\n",
        "#==============================================================================\n",
        "def perform_stationarity_analysis(ohlcv_data, df, name_map, fred_map, dependent_var):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 12: Performing Stationarity Analysis Using Augmented Dickey-Fuller...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    try:\n",
        "        #--> Sub-step 12.1: Identifying Key Variables for Analysis:\n",
        "        print(\"--> Sub-step 12.1: Identifying Key Variables for Analysis...\")\n",
        "        all_names = list(name_map.values())\n",
        "        stock_return_vars = [col for col in df.columns if col.startswith('_Daily_Log_Return_') and col.split('_')[-1] in all_names]\n",
        "        macro_vars = list(fred_map.keys())\n",
        "        all_key_vars = stock_return_vars + macro_vars\n",
        "        if dependent_var in df.columns:\n",
        "            all_key_vars.append(dependent_var)\n",
        "        print(f\"[SUCCESS] Identified {len(all_key_vars)} Key Variables for Stationarity Analysis.\")\n",
        "        print(\"[SUCCESS] Sub-step 12.1: Identifying Key Variables Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 12.2: Performing ADF Test on Key Variables:\n",
        "        print(\"\\n--> Sub-step 12.2: Performing ADF Test on Key Variables...\")\n",
        "        stationarity_results = []\n",
        "        for return_col in stock_return_vars:\n",
        "            symbol_name = return_col.split('_')[-1]\n",
        "            symbol = next((s for s, n in name_map.items() if n == symbol_name), None)\n",
        "            if not symbol or symbol not in ohlcv_data['Close'].columns: continue\n",
        "            price_series = ohlcv_data['Close'][symbol].dropna()\n",
        "            return_series = df[return_col].replace(0, np.nan).dropna()\n",
        "            if not price_series.empty:\n",
        "                stationarity_results.append({'Variable_Name': symbol, 'ADF_p_value': adfuller(price_series)[1], 'Series_Type': 'Price'})\n",
        "            if not return_series.empty:\n",
        "                stationarity_results.append({'Variable_Name': return_col, 'ADF_p_value': adfuller(return_series)[1], 'Series_Type': 'Log_Return'})\n",
        "            print(f\"[SUCCESS] Performed ADF Test for Price and Return Series of '{symbol_name}'.\")\n",
        "        for macro_col in macro_vars:\n",
        "            macro_series = df[macro_col].dropna()\n",
        "            if not macro_series.empty:\n",
        "                stationarity_results.append({'Variable_Name': macro_col, 'ADF_p_value': adfuller(macro_series)[1], 'Series_Type': 'Level'})\n",
        "                print(f\"[SUCCESS] Performed ADF Test for Macro Variable '{macro_col}'.\")\n",
        "        if dependent_var in df.columns:\n",
        "            dependent_series = df[dependent_var].replace(0, np.nan).dropna()\n",
        "            if not dependent_series.empty:\n",
        "                stationarity_results.append({'Variable_Name': dependent_var, 'ADF_p_value': adfuller(dependent_series)[1], 'Series_Type': 'Log_Return'})\n",
        "                print(f\"[SUCCESS] Performed ADF Test for Dependent Variable '{dependent_var}'.\")\n",
        "        print(\"[SUCCESS] Sub-step 12.2: Performing ADF Test Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 12.3: Reporting Statistics and Generating Output File:\n",
        "        print(\"\\n--> Sub-step 12.3: Reporting Statistics and Generating Output File...\")\n",
        "        results_df = pd.DataFrame(stationarity_results)\n",
        "        price_stats = results_df[results_df['Series_Type'] == 'Price']\n",
        "        return_stats = results_df[results_df['Series_Type'] == 'Log_Return']\n",
        "        macro_stats = results_df[results_df['Series_Type'] == 'Level']\n",
        "        pct_price_non_stationary = (price_stats['ADF_p_value'] >= 0.05).mean() * 100 if not price_stats.empty else 0\n",
        "        pct_return_stationary = (return_stats['ADF_p_value'] < 0.05).mean() * 100 if not return_stats.empty else 0\n",
        "        pct_macro_non_stationary = (macro_stats['ADF_p_value'] >= 0.05).mean() * 100 if not macro_stats.empty else 0\n",
        "        print(\"--- Stationarity Analysis Statistics ---\")\n",
        "        print(f\"1. Pct. of Price Series That Were Non-Stationary: {pct_price_non_stationary:.2f}%\")\n",
        "        print(f\"2. Pct. of Log-Return Series That Were Stationary: {pct_return_stationary:.2f}%\")\n",
        "        print(f\"3. Pct. of Macroeconomic Series That Were Non-Stationary: {pct_macro_non_stationary:.2f}%\")\n",
        "        results_df['Row'] = [to_persian_numerals(i) for i in range(1, len(results_df) + 1)]\n",
        "        results_df.to_csv('Stationarity_Results.csv', sep=',', index=False, encoding='utf-8-sig', columns=['Row', 'Variable_Name', 'ADF_p_value', 'Series_Type'])\n",
        "        print(\"[SUCCESS] Generated Output File: 'Stationarity_Results.csv'.\")\n",
        "        files.download('Stationarity_Results.csv')\n",
        "        print(\"[SUCCESS] Sub-step 12.3: Reporting Statistics and Generating Output File Completed Successfully.\")\n",
        "\n",
        "        print(\"\\n[SUCCESS] Step 12: Performing Stationarity Analysis Completed Successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] An Error Occurred During Stationarity Analysis: {e}\")\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 13: PERFORM CORRELATION ANALYSIS - PEARSON\n",
        "#==============================================================================\n",
        "def perform_correlation_analysis(df, dependent_var, name_map, fred_map):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 13: Performing Correlation Analysis Using Pearson...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    try:\n",
        "        #--> Sub-step 13.1: Identifying Key Variables for Analysis:\n",
        "        print(\"--> Sub-step 13.1: Identifying Key Variables for Analysis...\")\n",
        "        stock_vars = [f\"_Daily_Log_Return_{name}\" for name in name_map.values()]\n",
        "        macro_vars = list(fred_map.keys())\n",
        "        independent_vars = [col for col in stock_vars + macro_vars if col in df.columns]\n",
        "        print(f\"[SUCCESS] Identified 1 Dependent Variable and {len(independent_vars)} Key Independent Variables.\")\n",
        "        print(\"[SUCCESS] Sub-step 13.1: Identifying Key Variables Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 13.2: Performing Pearson Correlation Analysis:\n",
        "        print(\"\\n--> Sub-step 13.2: Performing Pearson Correlation Analysis...\")\n",
        "        correlation_results = []\n",
        "        for var in independent_vars:\n",
        "            subset_df = df[[dependent_var, var]][(df[dependent_var] != 0) & (df[var] != 0)]\n",
        "            if not subset_df.empty:\n",
        "                correlation = subset_df[dependent_var].corr(subset_df[var], method='pearson')\n",
        "                correlation_results.append({'Variable_Name': var, 'Pearson_Correlation': correlation})\n",
        "                print(f\"[SUCCESS] Calculated Correlation between Dependent Variable and '{var}'.\")\n",
        "        print(\"[SUCCESS] Sub-step 13.2: Performing Pearson Correlation Analysis Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 13.3: Generating Output File:\n",
        "        print(\"\\n--> Sub-step 13.3: Generating Output File...\")\n",
        "        results_df = pd.DataFrame(correlation_results).sort_values(by='Pearson_Correlation', ascending=False)\n",
        "        results_df['Row'] = [to_persian_numerals(i) for i in range(1, len(results_df) + 1)]\n",
        "        results_df.to_csv('Correlation_Results.csv', sep=',', index=False, encoding='utf-8-sig', columns=['Row', 'Variable_Name', 'Pearson_Correlation'])\n",
        "        print(\"[SUCCESS] Generated Output File: 'Correlation_Results.csv'.\")\n",
        "        files.download('Correlation_Results.csv')\n",
        "        print(\"[SUCCESS] Sub-step 13.3: Generating Output File Completed Successfully.\")\n",
        "\n",
        "        print(\"\\n[SUCCESS] Step 13: Performing Correlation Analysis Completed Successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] An Error Occurred During Correlation Analysis: {e}\")\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 14: OPTIMIZE DATA TYPES FOR MEMORY EFFICIENCY\n",
        "#==============================================================================\n",
        "def optimize_data_types(df):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 14: Optimizing Data Types for Memory Efficiency...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    optimization_df = df.copy()\n",
        "    try:\n",
        "        #--> Sub-step 14.1: Calculating Initial Memory Usage:\n",
        "        print(\"--> Sub-step 14.1: Calculating Initial Memory Usage...\")\n",
        "        initial_memory = optimization_df.memory_usage(deep=True).sum() / (1024**2)\n",
        "        print(f\"[SUCCESS] Initial DataFrame Memory Usage is {initial_memory:.2f} MB.\")\n",
        "        print(\"[SUCCESS] Sub-step 14.1: Calculating Initial Memory Usage Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 14.2: Optimizing Data Types:\n",
        "        print(\"\\n--> Sub-step 14.2: Optimizing Data Types...\")\n",
        "        inf_count = np.isinf(optimization_df.select_dtypes(include=np.number)).sum().sum()\n",
        "        if inf_count > 0: optimization_df.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "        for col in optimization_df.columns:\n",
        "            if optimization_df[col].dtype == 'float64': optimization_df[col] = optimization_df[col].astype('float32')\n",
        "            elif optimization_df[col].dtype == 'int64':\n",
        "                c_min, c_max = optimization_df[col].min(), optimization_df[col].max()\n",
        "                if c_min >= 0:\n",
        "                    if c_max < 256: optimization_df[col] = optimization_df[col].astype('uint8')\n",
        "                    elif c_max < 65536: optimization_df[col] = optimization_df[col].astype('uint16')\n",
        "                else:\n",
        "                    if c_min > -128 and c_max < 128: optimization_df[col] = optimization_df[col].astype('int8')\n",
        "            print(f\"[SUCCESS] Optimized Data Type for Column '{col}'.\")\n",
        "        print(\"[SUCCESS] Sub-step 14.2: Optimizing Data Types Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 14.3: Calculating and Reporting Final Statistics:\n",
        "        print(\"\\n--> Sub-step 14.3: Calculating and Reporting Final Statistics...\")\n",
        "        final_memory = optimization_df.memory_usage(deep=True).sum() / (1024**2)\n",
        "        reduction_pct = (initial_memory - final_memory) / initial_memory * 100 if initial_memory > 0 else 0\n",
        "        print(\"--- Memory Optimization Statistics ---\")\n",
        "        print(f\"1. Initial Memory Usage: {initial_memory:.2f} MB\")\n",
        "        print(f\"2. Final Memory Usage: {final_memory:.2f} MB\")\n",
        "        print(f\"3. Memory Reduction: {(initial_memory - final_memory):.2f} MB ({reduction_pct:.2f}%)\")\n",
        "        print(\"[SUCCESS] Sub-step 14.3: Calculating and Reporting Final Statistics Completed Successfully.\")\n",
        "\n",
        "        print(\"\\n[SUCCESS] Step 14: Optimizing Data Types Completed Successfully.\")\n",
        "        return optimization_df\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] An Error Occurred During Data Type Optimization: {e}\")\n",
        "        return df\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 15: FINALIZE DATA STRUCTURE AND COLUMN ORDER\n",
        "#==============================================================================\n",
        "def finalize_data_structure(df, company_info_df, name_map, fred_map, yfinance_map, dependent_var):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 15: Finalizing Data Structure and Column Order...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    try:\n",
        "        #--> Sub-step 15.1: Finalizing Column Order:\n",
        "        print(\"--> Sub-step 15.1: Finalizing Column Order...\")\n",
        "        final_column_order = ['_Date', '_Year', '_Month', '_Day', '_WeekOfYear', '_DayOfWeek']\n",
        "        feature_names = [\n",
        "            '_Daily_Log_Open', '_Daily_Log_High', '_Daily_Log_Low', '_Daily_Log_Close', '_Daily_Log_Volume',\n",
        "            '_Daily_Log_Return', '_SMA_20', '_SMA_50', '_EMA_20', '_EMA_50', '_ADX_14', '_RSI_14',\n",
        "            '_MACD_line', '_MACD_signal', '_MACD_hist', '_BB_upper', '_BB_mid', '_BB_lower', '_ATR_14',\n",
        "            '_OBV', '_MFI_14', '_HV_5', '_HV_21', '_HV_63'\n",
        "        ]\n",
        "        unique_symbols = sorted(company_info_df['Symbol'].unique())\n",
        "        for symbol in unique_symbols:\n",
        "            company_name_suffix = f\"_{name_map.get(symbol, symbol)}\"\n",
        "            for feature in feature_names: final_column_order.append(f\"{feature}{company_name_suffix}\")\n",
        "        final_column_order.extend(sorted(list(fred_map.keys())))\n",
        "        final_column_order.extend(sorted(list(yfinance_map.keys())))\n",
        "        final_column_order.extend(sorted(['_EO', '_ND', '_PSI', '_TWI']))\n",
        "        if dependent_var in df.columns: final_column_order.append(dependent_var)\n",
        "        existing_columns_in_order = [col for col in final_column_order if col in df.columns]\n",
        "        final_df = df[existing_columns_in_order]\n",
        "        print(\"[SUCCESS] Final Column Order Has Been Applied.\")\n",
        "        print(\"[SUCCESS] Sub-step 15.1: Finalizing Column Order Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 15.2: Reporting Final DataFrame Statistics:\n",
        "        print(\"\\n--> Sub-step 15.2: Reporting Final DataFrame Statistics...\")\n",
        "        num_rows = len(final_df)\n",
        "        num_cols = len(final_df.columns)\n",
        "        total_data_points = num_rows * num_cols\n",
        "        numeric_df = final_df.select_dtypes(include=[np.number])\n",
        "        num_empty_rows = (numeric_df == 0).all(axis=1).sum()\n",
        "        num_empty_cols = (numeric_df == 0).all(axis=0).sum()\n",
        "        print(\"--- Final DataFrame Statistics ---\")\n",
        "        print(f\"1. Total Number of Rows: {num_rows}\")\n",
        "        print(f\"2. Total Number of Columns: {num_cols}\")\n",
        "        print(f\"3. Total Data Points: {total_data_points}\")\n",
        "        print(f\"4. Completely Empty Rows (All Zeros): {num_empty_rows}\")\n",
        "        print(f\"5. Completely Empty Columns (All Zeros): {num_empty_cols}\")\n",
        "        print(\"[SUCCESS] Sub-step 15.2: Reporting Final DataFrame Statistics Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 15.3: Generating Final Data File:\n",
        "        print(\"\\n--> Sub-step 15.3: Generating Final Data File...\")\n",
        "        final_df.to_csv('Thesis_Data_File.csv', sep=',', index=False, date_format='%Y-%m-%d')\n",
        "        print(\"[SUCCESS] Generated Final Data File: 'Thesis_Data_File.csv'.\")\n",
        "        files.download('Thesis_Data_File.csv')\n",
        "        print(\"[SUCCESS] Sub-step 15.3: Generating Final Data File Completed Successfully.\")\n",
        "\n",
        "        print(\"\\n[SUCCESS] Step 15: Finalizing Data Structure and Column Order Completed Successfully.\")\n",
        "        return final_df\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] An Error Occurred During Data Structure Finalization: {e}\")\n",
        "        return df\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 16: GENERATE DESCRIPTIVE STATISTICS FOR KEY VARIABLES\n",
        "#==============================================================================\n",
        "def generate_descriptive_statistics(df, dependent_var, name_map, fred_map):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 16: Generating Descriptive Statistics for Key Variables...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    try:\n",
        "        def get_stats(series, name):\n",
        "            series = series[series != 0]\n",
        "            if series.empty: return None\n",
        "            return {'Variable_Name': name, 'Mean': series.mean(), 'Median': series.median(),\n",
        "                    'Std_Dev': series.std(), 'Skewness': series.skew(), 'Kurtosis': series.kurt(),\n",
        "                    'Min': series.min(), 'Max': series.max(), 'Count': series.count()}\n",
        "\n",
        "        #--> Sub-step 16.1: Generating Statistics for Dependent Variable:\n",
        "        print(\"--> Sub-step 16.1: Generating Statistics for Dependent Variable...\")\n",
        "        dep_stats = get_stats(df[dependent_var], dependent_var) if dependent_var in df.columns else None\n",
        "        if dep_stats: print(f\"[SUCCESS] Generated Statistics for '{dependent_var}'.\")\n",
        "        print(\"[SUCCESS] Sub-step 16.1: Generating Statistics for Dependent Variable Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 16.2: Generating Statistics for Key Independent Variables:\n",
        "        print(\"\\n--> Sub-step 16.2: Generating Statistics for Key Independent Variables...\")\n",
        "        stock_vars = [f\"_Daily_Log_Return_{name}\" for name in name_map.values()]\n",
        "        macro_vars = list(fred_map.keys())\n",
        "        independent_vars = [col for col in stock_vars + macro_vars if col in df.columns]\n",
        "        indep_stats_list = []\n",
        "        for col in independent_vars:\n",
        "            stats = get_stats(df[col], col)\n",
        "            if stats is not None:\n",
        "                indep_stats_list.append(stats)\n",
        "                print(f\"[SUCCESS] Generated Statistics for '{col}'.\")\n",
        "        print(\"[SUCCESS] Sub-step 16.2: Generating Statistics for Key Independent Variables Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 16.3: Generating Output Files:\n",
        "        print(\"\\n--> Sub-step 16.3: Generating Output Files...\")\n",
        "        if dep_stats:\n",
        "            dep_df = pd.DataFrame([dep_stats])\n",
        "            dep_df['Row'] = to_persian_numerals(1)\n",
        "            dep_df.to_csv('Dependent_Variable_Statistics.csv', sep=',', index=False, encoding='utf-8-sig', columns=['Row'] + [c for c in dep_df.columns if c != 'Row'])\n",
        "            print(\"[SUCCESS] Generated Output File: 'Dependent_Variable_Statistics.csv'.\")\n",
        "            files.download('Dependent_Variable_Statistics.csv'); time.sleep(1)\n",
        "        if indep_stats_list:\n",
        "            indep_df = pd.DataFrame(indep_stats_list)\n",
        "            indep_df['Row'] = [to_persian_numerals(i) for i in range(1, len(indep_df) + 1)]\n",
        "            indep_df.to_csv('Independent_Variables_Statistics.csv', sep=',', index=False, encoding='utf-8-sig', columns=['Row'] + [c for c in indep_df.columns if c != 'Row'])\n",
        "            print(\"[SUCCESS] Generated Output File: 'Independent_Variables_Statistics.csv'.\")\n",
        "            files.download('Independent_Variables_Statistics.csv')\n",
        "        print(\"[SUCCESS] Sub-step 16.3: Generating Output Files Completed Successfully.\")\n",
        "\n",
        "        print(\"\\n[SUCCESS] Step 16: Generating Descriptive Statistics Completed Successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] An Error Occurred During Descriptive Statistics Generation: {e}\")\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 17: SPLIT DATASET INTO CHRONOLOGICAL SETS\n",
        "#==============================================================================\n",
        "def split_dataset(df):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 17: Splitting Dataset into Chronological Sets...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    try:\n",
        "        #--> Sub-step 17.1: Performing Chronological Data Split:\n",
        "        print(\"--> Sub-step 17.1: Performing Chronological Data Split...\")\n",
        "        n_total = len(df)\n",
        "        train_end_idx = int(n_total * 0.70)\n",
        "        val_end_idx = train_end_idx + int(n_total * 0.15)\n",
        "        train_df = df.iloc[:train_end_idx].copy()\n",
        "        val_df = df.iloc[train_end_idx:val_end_idx].copy()\n",
        "        test_df = df.iloc[val_end_idx:].copy()\n",
        "        print(\"[SUCCESS] Chronological Data Split Has Been Performed.\")\n",
        "        print(\"[SUCCESS] Sub-step 17.1: Performing Chronological Data Split Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 17.2: Reporting Split Statistics:\n",
        "        print(\"\\n--> Sub-step 17.2: Reporting Split Statistics...\")\n",
        "        print(\"--- Data Split Statistics ---\")\n",
        "        print(f\"1. Training Set (70%):   {train_df['_Date'].min().date()} to {train_df['_Date'].max().date()} ({len(train_df)} Rows)\")\n",
        "        print(f\"2. Validation Set (15%): {val_df['_Date'].min().date()} to {val_df['_Date'].max().date()} ({len(val_df)} Rows)\")\n",
        "        print(f\"3. Test Set (15%):       {test_df['_Date'].min().date()} to {test_df['_Date'].max().date()} ({len(test_df)} Rows)\")\n",
        "        print(\"[SUCCESS] Sub-step 17.2: Reporting Split Statistics Completed Successfully.\")\n",
        "\n",
        "        print(\"\\n[SUCCESS] Step 17: Splitting Dataset into Time-Based Sets Completed Successfully.\")\n",
        "        return train_df, val_df, test_df\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] An Error Occurred During Dataset Splitting: {e}\")\n",
        "        return None, None, None\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 18: PERFORM DATA STANDARDIZATION - Z-SCORE\n",
        "#==============================================================================\n",
        "def perform_data_standardization(train_df, val_df, test_df, dependent_var):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 18: Performing Data Standardization Using Z-Score...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    try:\n",
        "        #--> Sub-step 18.1: Identifying Variables for Standardization:\n",
        "        print(\"--> Sub-step 18.1: Identifying Variables for Standardization...\")\n",
        "        feature_cols = train_df.columns.drop(['_Date', dependent_var])\n",
        "        print(f\"[SUCCESS] Identified {len(feature_cols)} Variables for Standardization.\")\n",
        "        print(\"[SUCCESS] Sub-step 18.1: Identifying Variables Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 18.2: Performing Standardization on Data Sets:\n",
        "        print(\"\\n--> Sub-step 18.2: Performing Standardization on Data Sets...\")\n",
        "        scaler = StandardScaler()\n",
        "        train_scaled = train_df.copy(); val_scaled = val_df.copy(); test_scaled = test_df.copy()\n",
        "        train_scaled[feature_cols] = scaler.fit_transform(train_df[feature_cols])\n",
        "        print(\"[SUCCESS] Standardized Training Set.\")\n",
        "        val_scaled[feature_cols] = scaler.transform(val_df[feature_cols])\n",
        "        print(\"[SUCCESS] Standardized Validation Set.\")\n",
        "        test_scaled[feature_cols] = scaler.transform(test_df[feature_cols])\n",
        "        print(\"[SUCCESS] Standardized Test Set.\")\n",
        "        print(\"[SUCCESS] Sub-step 18.2: Performing Standardization on Data Sets Completed Successfully.\")\n",
        "\n",
        "        print(\"\\n[SUCCESS] Step 18: Performing Data Standardization Completed Successfully.\")\n",
        "        return train_scaled, val_scaled, test_scaled\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] An Error Occurred During Data Standardization: {e}\")\n",
        "        return None, None, None\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 19: BUILD BASELINE MODEL - MULTIVARIATE LINEAR REGRESSION\n",
        "#==============================================================================\n",
        "def build_baseline_model(train_df, test_df, dependent_var, name_map, fred_map):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 19: Building Baseline Model...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    try:\n",
        "        #--> Sub-step 19.1: Identifying Model Variables:\n",
        "        print(\"--> Sub-step 19.1: Identifying Model Variables...\")\n",
        "        stock_vars = [f\"_Daily_Log_Return_{name}\" for name in name_map.values()]\n",
        "        macro_vars = list(fred_map.keys())\n",
        "        independent_vars = [col for col in stock_vars + macro_vars if col in train_df.columns]\n",
        "        print(f\"[SUCCESS] Identified 1 Dependent Variable and {len(independent_vars)} Key Independent Variables.\")\n",
        "        print(\"[SUCCESS] Sub-step 19.1: Identifying Model Variables Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 19.2: Building and Evaluating Model:\n",
        "        print(\"\\n--> Sub-step 19.2: Building and Evaluating Model...\")\n",
        "        X_train, y_train = train_df[independent_vars], train_df[dependent_var]\n",
        "        X_test, y_test = test_df[independent_vars], test_df[dependent_var]\n",
        "        baseline_model = LinearRegression()\n",
        "        baseline_model.fit(X_train, y_train)\n",
        "        y_pred = baseline_model.predict(X_test)\n",
        "        print(\"[SUCCESS] Baseline Model Has Been Built and Evaluated.\")\n",
        "        print(\"[SUCCESS] Sub-step 19.2: Building and Evaluating Model Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 19.3: Reporting Performance and Generating Output File:\n",
        "        print(\"\\n--> Sub-step 19.3: Reporting Performance and Generating Output File...\")\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        mape = np.mean(np.abs((y_test - y_pred) / np.where(y_test == 0, 1e-9, y_test))) * 100\n",
        "        da = np.mean(np.sign(y_test) == np.sign(y_pred)) * 100\n",
        "        print(\"--- Baseline Model Performance ---\")\n",
        "        print(f\"1. RMSE: {rmse:.4f}\")\n",
        "        print(f\"2. MAE: {mae:.4f}\")\n",
        "        print(f\"3. R2_Score: {r2:.4f}\")\n",
        "        print(f\"4. MAPE: {mape:.2f}%\")\n",
        "        print(f\"5. Directional_Accuracy: {da:.2f}%\")\n",
        "        results_df = pd.DataFrame([{'Model_Name': 'Baseline_Linear_Regression', 'RMSE': rmse, 'MAE': mae, 'R2_Score': r2, 'MAPE': mape, 'Directional_Accuracy': da}])\n",
        "        results_df.insert(0, 'Row', to_persian_numerals(1))\n",
        "        results_df.to_csv('Baseline_Model_Performance.csv', sep=',', index=False, encoding='utf-8-sig')\n",
        "        print(\"[SUCCESS] Generated Output File: 'Baseline_Model_Performance.csv'.\")\n",
        "        files.download('Baseline_Model_Performance.csv')\n",
        "        print(\"[SUCCESS] Sub-step 19.3: Reporting Performance and Generating Output File Completed Successfully.\")\n",
        "\n",
        "        print(\"\\n[SUCCESS] Step 19: Building Baseline Model Completed Successfully.\")\n",
        "        return baseline_model\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] An Error Occurred During Baseline Model Building: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 20: DISCOVER OPTIMAL MODEL WITH H2O AUTOML\n",
        "#==============================================================================\n",
        "def discover_optimal_model_with_h2o(train_df, val_df, test_df, dependent_var):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 20: Discovering Optimal Model with H2O AutoML...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    try:\n",
        "        h2o.init(nthreads=-1, max_mem_size=None, log_level=\"ERRR\")\n",
        "        excluded_algos = [\"StackedEnsemble\", \"DeepLearning\"]\n",
        "        test_max_runtime_secs = 3600\n",
        "\n",
        "        #--> Sub-step 20.1: Reporting AutoML Configuration and Resources:\n",
        "        print(\"--> Sub-step 20.1: Reporting AutoML Configuration and Resources...\")\n",
        "        print(\"\\n--- AutoML Configuration (TEST RUN) ---\")\n",
        "        print(f\"1. Max Runtime: {test_max_runtime_secs} seconds ({test_max_runtime_secs//60} minutes)\")\n",
        "        print(\"2. Max Models: Unlimited (Stops by Time or Early Stopping)\")\n",
        "        print(\"3. Stopping Metric: RMSE\")\n",
        "        print(\"4. Stopping Rounds: 10\")\n",
        "        print(f\"5. Excluded Algorithms: {excluded_algos}\")\n",
        "        total_ram_gb = psutil.virtual_memory().total / (1024**3)\n",
        "        h2o_ram_gb = h2o.cluster().nodes[0]['free_mem'] / (1024**3)\n",
        "        print(\"\\n--- Computational Resource Statistics ---\")\n",
        "        print(f\"1. Total System CPU Cores: {psutil.cpu_count(logical=True)}\")\n",
        "        print(f\"2. Total System RAM: {total_ram_gb:.2f} GB\")\n",
        "        print(f\"3. Memory Allocated to H2O: {h2o_ram_gb:.2f} GB\")\n",
        "        print(\"[SUCCESS] Sub-step 20.1: Reporting AutoML Configuration and Resources Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 20.2: Discovering Optimal Model via AutoML Training:\n",
        "        print(\"\\n--> Sub-step 20.2: Discovering Optimal Model via AutoML Training...\")\n",
        "        train_h2o = h2o.H2OFrame(train_df); val_h2o = h2o.H2OFrame(val_df); test_h2o = h2o.H2OFrame(test_df)\n",
        "        independent_vars = [col for col in train_df.columns if col not in [dependent_var, '_Date']]\n",
        "        aml = H2OAutoML(nfolds=0, max_runtime_secs=test_max_runtime_secs, max_models=0, stopping_metric=\"RMSE\", stopping_rounds=10, sort_metric=\"RMSE\", seed=1, exclude_algos=excluded_algos)\n",
        "        aml.train(x=independent_vars, y=dependent_var, training_frame=train_h2o, validation_frame=val_h2o)\n",
        "        print(\"[SUCCESS] AutoML Training Has Completed.\")\n",
        "        print(\"[SUCCESS] Sub-step 20.2: Discovering Optimal Model via AutoML Training Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 20.3: Evaluating All Models and Generating Leaderboard File:\n",
        "        print(\"\\n--> Sub-step 20.3: Evaluating All Models and Generating Leaderboard File...\")\n",
        "        leaderboard = aml.leaderboard.as_data_frame(use_multi_thread=True)\n",
        "        all_models_perf = []\n",
        "        y_test_pd = test_df[dependent_var].values\n",
        "        for model_id in leaderboard['model_id']:\n",
        "            model = h2o.get_model(model_id)\n",
        "            perf_test = model.model_performance(test_h2o)\n",
        "            preds = model.predict(test_h2o).as_data_frame(use_multi_thread=True)['predict'].values\n",
        "            rmse = perf_test.rmse()\n",
        "            mae = perf_test.mae()\n",
        "            r2 = perf_test.r2()\n",
        "            mape = np.mean(np.abs((y_test_pd - preds) / np.where(y_test_pd == 0, 1e-9, y_test_pd))) * 100\n",
        "            da = np.mean(np.sign(y_test_pd) == np.sign(preds)) * 100\n",
        "            all_models_perf.append({'Model_Name': model_id, 'RMSE': rmse, 'MAE': mae, 'R2_Score': r2, 'MAPE': mape, 'Directional_Accuracy': da})\n",
        "        leaderboard_df = pd.DataFrame(all_models_perf)\n",
        "        leaderboard_df['Row'] = [to_persian_numerals(i) for i in range(1, len(leaderboard_df) + 1)]\n",
        "        leaderboard_df.to_csv('H2O_AutoML_Leaderboard.csv', sep=',', index=False, encoding='utf-8-sig', columns=['Row', 'Model_Name', 'RMSE', 'MAE', 'R2_Score', 'MAPE', 'Directional_Accuracy'])\n",
        "        print(\"[SUCCESS] Generated Output File: 'H2O_AutoML_Leaderboard.csv'.\")\n",
        "        files.download('H2O_AutoML_Leaderboard.csv')\n",
        "        print(\"[SUCCESS] Sub-step 20.3: Evaluating All Models and Generating Leaderboard File Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 20.4: Evaluating Best Model and Generating Output Files:\n",
        "        print(\"\\n--> Sub-step 20.4: Evaluating Best Model and Generating Output Files...\")\n",
        "        best_model = aml.leader\n",
        "        perf_test = best_model.model_performance(test_h2o)\n",
        "        y_true_test = test_df[dependent_var].values\n",
        "        y_pred_test = best_model.predict(test_h2o).as_data_frame(use_multi_thread=True)['predict'].values\n",
        "        mape = np.mean(np.abs((y_true_test - y_pred_test) / np.where(y_true_test == 0, 1e-9, y_true_test))) * 100\n",
        "        da = np.mean(np.sign(y_true_test) == np.sign(y_pred_test)) * 100\n",
        "        best_model_perf = pd.DataFrame([{'Model_Name': best_model.model_id, 'RMSE_Test': perf_test.rmse(), 'MAE_Test': perf_test.mae(), 'R2_Test': perf_test.r2(), 'MAPE_Test': mape, 'Directional_Accuracy_Test': da}])\n",
        "        best_model_perf.insert(0, 'Row', to_persian_numerals(1))\n",
        "        best_model_perf.to_csv('H2O_Best_Model_Test_Performance.csv', sep=',', index=False, encoding='utf-8-sig', columns=['Row', 'Model_Name', 'RMSE_Test', 'MAE_Test', 'R2_Test', 'MAPE_Test', 'Directional_Accuracy_Test'])\n",
        "        print(\"[SUCCESS] Generated Output File: 'H2O_Best_Model_Test_Performance.csv'.\")\n",
        "        files.download('H2O_Best_Model_Test_Performance.csv')\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.scatter(y_true_test, y_pred_test, alpha=0.6, color=COLOR_PALETTE['Color4'])\n",
        "        plt.plot([y_true_test.min(), y_true_test.max()], [y_true_test.min(), y_true_test.max()], color=COLOR_PALETTE['Color2'], linestyle='--')\n",
        "        plt.title('Actual vs. Predicted Values'); plt.xlabel('Actual Values'); plt.ylabel('Predicted Values'); plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
        "        plt.gca().set_facecolor(COLOR_PALETTE['Color3'])\n",
        "        plt.savefig('Scatter_Plot_Actual_vs_Predicted.png', dpi=300)\n",
        "        print(\"[SUCCESS] Generated Output File: 'Scatter_Plot_Actual_vs_Predicted.png'.\")\n",
        "        files.download('Scatter_Plot_Actual_vs_Predicted.png')\n",
        "        print(\"[SUCCESS] Sub-step 20.4: Evaluating Best Model and Generating Output Files Completed Successfully.\")\n",
        "\n",
        "        print(\"\\n[SUCCESS] Step 20: Discovering Optimal Model with H2O AutoML Completed Successfully.\")\n",
        "        return best_model, leaderboard\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] An Error Occurred During H2O AutoML Process: {e}\")\n",
        "        return None, None\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 21: PERFORM STATISTICAL SIGNIFICANCE - DIEBOLD-MARIANO\n",
        "#==============================================================================\n",
        "def diebold_mariano_test(actuals, preds1, preds2):\n",
        "    loss_diff = np.square(actuals - preds1) - np.square(actuals - preds2)\n",
        "    d_mean = np.mean(loss_diff)\n",
        "    d_var = np.var(loss_diff, ddof=0)\n",
        "    if d_var == 0: return 0, 1.0\n",
        "    dm_statistic = d_mean / np.sqrt(d_var / len(actuals))\n",
        "    p_value = 2 * (1 - norm.cdf(np.abs(dm_statistic)))\n",
        "    return dm_statistic, p_value\n",
        "\n",
        "def perform_statistical_significance_test(baseline_model, best_h2o_model, h2o_leaderboard, train_df, test_df, dependent_var, independent_vars):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 21: Performing Statistical Significance Using Diebold-Mariano...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    try:\n",
        "        #--> Sub-step 21.1: Preparing Predictions for All Models:\n",
        "        print(\"--> Sub-step 21.1: Preparing Predictions for All Models...\")\n",
        "        y_true = test_df[dependent_var].values\n",
        "        baseline_model.fit(train_df[independent_vars], train_df[dependent_var])\n",
        "        baseline_preds = baseline_model.predict(test_df[independent_vars])\n",
        "        best_h2o_preds = best_h2o_model.predict(h2o.H2OFrame(test_df)).as_data_frame(use_multi_thread=True)['predict'].values\n",
        "        print(\"[SUCCESS] All Model Predictions Are Ready.\")\n",
        "        print(\"[SUCCESS] Sub-step 21.1: Preparing Predictions for All Models Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 21.2: Performing Diebold-Mariano Test: Baseline vs. H2O Models:\n",
        "        print(\"\\n--> Sub-step 21.2: Performing Diebold-Mariano Test: Baseline vs. H2O Models...\")\n",
        "        dm_results_baseline = []\n",
        "        for model_id in h2o_leaderboard['model_id']:\n",
        "            h2o_model = h2o.get_model(model_id)\n",
        "            h2o_preds = h2o_model.predict(h2o.H2OFrame(test_df)).as_data_frame(use_multi_thread=True)['predict'].values\n",
        "            dm_stat, p_val = diebold_mariano_test(y_true, baseline_preds, h2o_preds)\n",
        "            comparison_name = f\"Baseline_vs_{model_id}\"\n",
        "            dm_results_baseline.append({'Comparison': comparison_name, 'DM_Statistic': dm_stat, 'P_Value': p_val})\n",
        "            print(f\"[SUCCESS] Performed Diebold-Mariano Test for Comparison: '{comparison_name}'.\")\n",
        "        print(\"[SUCCESS] Sub-step 21.2: Performing Diebold-Mariano Test: Baseline vs. H2O Models Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 21.3: Performing Diebold-Mariano Test: Best H2O vs. Other H2O Models:\n",
        "        print(\"\\n--> Sub-step 21.3: Performing Diebold-Mariano Test: Best H2O vs. Other H2O Models...\")\n",
        "        dm_results_best = []\n",
        "        best_model_id = best_h2o_model.model_id\n",
        "        for model_id in h2o_leaderboard['model_id']:\n",
        "            if model_id == best_model_id: continue\n",
        "            other_model = h2o.get_model(model_id)\n",
        "            other_preds = other_model.predict(h2o.H2OFrame(test_df)).as_data_frame(use_multi_thread=True)['predict'].values\n",
        "            dm_stat, p_val = diebold_mariano_test(y_true, other_preds, best_h2o_preds)\n",
        "            comparison_name = f\"{best_model_id}_vs_{model_id}\"\n",
        "            dm_results_best.append({'Comparison': comparison_name, 'DM_Statistic': dm_stat, 'P_Value': p_val})\n",
        "            print(f\"[SUCCESS] Performed Diebold-Mariano Test for Comparison: '{comparison_name}'.\")\n",
        "        print(\"[SUCCESS] Sub-step 21.3: Performing Diebold-Mariano Test: Best H2O vs. Other H2O Models Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 21.4: Generating Output Files:\n",
        "        print(\"\\n--> Sub-step 21.4: Generating Output Files...\")\n",
        "        results_baseline_df = pd.DataFrame(dm_results_baseline)\n",
        "        results_baseline_df['Row'] = [to_persian_numerals(i) for i in range(1, len(results_baseline_df) + 1)]\n",
        "        results_baseline_df.to_csv('Diebold_Mariano_Baseline_vs_H2O.csv', sep=',', index=False, encoding='utf-8-sig', columns=['Row', 'Comparison', 'DM_Statistic', 'P_Value'])\n",
        "        print(\"[SUCCESS] Generated Output File: 'Diebold_Mariano_Baseline_vs_H2O.csv'.\")\n",
        "        files.download('Diebold_Mariano_Baseline_vs_H2O.csv')\n",
        "        results_best_df = pd.DataFrame(dm_results_best)\n",
        "        results_best_df['Row'] = [to_persian_numerals(i) for i in range(1, len(results_best_df) + 1)]\n",
        "        results_best_df.to_csv('Diebold_Mariano_Best_vs_Others.csv', sep=',', index=False, encoding='utf-8-sig', columns=['Row', 'Comparison', 'DM_Statistic', 'P_Value'])\n",
        "        print(\"[SUCCESS] Generated Output File: 'Diebold_Mariano_Best_vs_Others.csv'.\")\n",
        "        files.download('Diebold_Mariano_Best_vs_Others.csv')\n",
        "        print(\"[SUCCESS] Sub-step 21.4: Generating Output Files Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 21.5: Reporting Final Statistics:\n",
        "        print(\"\\n--> Sub-step 21.5: Reporting Final Statistics...\")\n",
        "        best_vs_baseline_pval = results_baseline_df[results_baseline_df['Comparison'] == f\"Baseline_vs_{best_model_id}\"]['P_Value'].iloc[0]\n",
        "        best_vs_others_count = (results_best_df['P_Value'] < 0.05).sum()\n",
        "        print(\"\\n--- Diebold-Mariano Test Statistics ---\")\n",
        "        print(f\"1. Best H2O Model Statistically Superior to Baseline: {'Yes' if best_vs_baseline_pval < 0.05 else 'No'} (p-value: {best_vs_baseline_pval:.4f})\")\n",
        "        print(f\"2. Best H2O Model Statistically Superior to {best_vs_others_count} out of {len(h2o_leaderboard)-1} other H2O models.\")\n",
        "        print(\"[SUCCESS] Sub-step 21.5: Reporting Final Statistics Completed Successfully.\")\n",
        "\n",
        "        print(\"\\n[SUCCESS] Step 21: Performing Statistical Significance Completed Successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] An Error Occurred During Statistical Significance Analysis: {e}\")\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 22: GENERATE FEATURE IMPORTANCE REPORT FOR KEY VARIABLES\n",
        "#==============================================================================\n",
        "def generate_feature_importance_report(best_model, name_map, fred_map):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 22: Generating Feature Importance Report for Key Variables...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    try:\n",
        "        #--> Sub-step 22.1: Calculating Feature Importance:\n",
        "        print(\"--> Sub-step 22.1: Calculating Feature Importance...\")\n",
        "        algo = best_model.algo\n",
        "        feature_importance = None\n",
        "        if algo == 'glm':\n",
        "            coef_dict = best_model.coef_norm()\n",
        "            coef_df = pd.DataFrame(coef_dict.items(), columns=['variable', 'scaled_importance'])\n",
        "            coef_df['scaled_importance'] = coef_df['scaled_importance'].abs()\n",
        "            feature_importance = coef_df\n",
        "        elif algo in ['gbm', 'drf', 'xgboost'] and hasattr(best_model, 'varimp'):\n",
        "            feature_importance = best_model.varimp(use_pandas=True)\n",
        "        else:\n",
        "            print(f\"[ERROR] Feature Importance is not implemented for the '{algo}' model. Skipping This Step.\")\n",
        "            print(\"===========================================================================================\")\n",
        "            return\n",
        "        print(\"[SUCCESS] Feature Importance Has Been Calculated.\")\n",
        "        print(\"[SUCCESS] Sub-step 22.1: Calculating Feature Importance Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 22.2: Preparing Report Data:\n",
        "        print(\"\\n--> Sub-step 22.2: Preparing Report Data...\")\n",
        "        stock_vars = [f\"_Daily_Log_Return_{name}\" for name in name_map.values()]\n",
        "        macro_vars = list(fred_map.keys())\n",
        "        key_independent_vars = stock_vars + macro_vars\n",
        "        report_data = feature_importance[feature_importance['variable'].isin(key_independent_vars)].sort_values('scaled_importance', ascending=False)\n",
        "        print(f\"[SUCCESS] Data for {len(report_data)} Key Independent Variables Has Been Prepared.\")\n",
        "        print(\"[SUCCESS] Sub-step 22.2: Preparing Report Data Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 22.3: Generating Feature Importance File:\n",
        "        print(\"\\n--> Sub-step 22.3: Generating Feature Importance File...\")\n",
        "        csv_df = report_data[['variable', 'scaled_importance']].copy()\n",
        "        csv_df.columns = ['Variable_Name', 'Importance']\n",
        "        csv_df['Row'] = [to_persian_numerals(i) for i in range(1, len(csv_df) + 1)]\n",
        "        csv_df.to_csv('Feature_Importance_Results.csv', sep=',', index=False, encoding='utf-8-sig', columns=['Row', 'Variable_Name', 'Importance'])\n",
        "        print(\"[SUCCESS] Generated Output File: 'Feature_Importance_Results.csv'.\")\n",
        "        files.download('Feature_Importance_Results.csv')\n",
        "        print(\"[SUCCESS] Sub-step 22.3: Generating Feature Importance File Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 22.4: Generating Feature Importance Plot:\n",
        "        print(\"\\n--> Sub-step 22.4: Generating Feature Importance Plot...\")\n",
        "        plot_data = report_data.sort_values('scaled_importance', ascending=True)\n",
        "        plot_data['cleaned_variable'] = plot_data['variable'].str.replace('_Daily_Log_Return_', '').str.lstrip('_')\n",
        "        palette_cycle = [COLOR_PALETTE['Color4'], COLOR_PALETTE['Color5'], COLOR_PALETTE['Color6']]\n",
        "        colors = [palette_cycle[i % len(palette_cycle)] for i in range(len(plot_data))]\n",
        "        plt.figure(figsize=(12, max(10, 0.25 * len(plot_data))))\n",
        "        plt.barh(plot_data['cleaned_variable'], plot_data['scaled_importance'], color=colors)\n",
        "        plt.xlabel('Scaled Importance'); plt.title('Feature Importance for Key Independent Variables'); plt.grid(True, axis='x', linestyle='--', linewidth=0.5)\n",
        "        plt.gca().set_facecolor(COLOR_PALETTE['Color3'])\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('Feature_Importance.png', dpi=300)\n",
        "        print(\"[SUCCESS] Generated Output File: 'Feature_Importance.png'.\")\n",
        "        files.download('Feature_Importance.png')\n",
        "        print(\"[SUCCESS] Sub-step 22.4: Generating Feature Importance Plot Completed Successfully.\")\n",
        "\n",
        "        print(\"\\n[SUCCESS] Step 22: Generating Feature Importance Report Completed Successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] An Error Occurred During Feature Importance Report Generation: {e}\")\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#==============================================================================\n",
        "# STEP 23: GENERATE SHAP SUMMARY PLOT FOR KEY VARIABLES\n",
        "#==============================================================================\n",
        "def generate_shap_summary_plot(best_model, train_df, test_df, name_map, fred_map):\n",
        "    print(\"\\n===========================================================================================\")\n",
        "    print(\"STEP 23: Generating SHAP Summary Plot for Key Variables...\")\n",
        "    print(\"===========================================================================================\")\n",
        "    try:\n",
        "        #--> Sub-step 23.1: Calculating SHAP Values:\n",
        "        print(\"--> Sub-step 23.1: Calculating SHAP Values...\")\n",
        "        if not hasattr(best_model, 'predict_contributions'):\n",
        "            print(\"[ERROR] The Best Model Does Not Support Native SHAP Contributions. Skipping This Step.\")\n",
        "            print(\"===========================================================================================\")\n",
        "            return\n",
        "        test_h2o = h2o.H2OFrame(test_df)\n",
        "        algo = best_model.algo\n",
        "        if algo == 'glm':\n",
        "            print(\"Model is GLM, creating a summarized background frame for SHAP calculation...\")\n",
        "            independent_vars = [col for col in train_df.columns if col not in [DEPENDENT_VARIABLE, '_Date']]\n",
        "            train_for_summary = train_df[independent_vars]\n",
        "            train_summary_data = shap.kmeans(train_for_summary, 100)\n",
        "            train_summary_df = pd.DataFrame(train_summary_data.data, columns=train_for_summary.columns)\n",
        "            background_h2o = h2o.H2OFrame(train_summary_df)\n",
        "            shap_values_h2o = best_model.predict_contributions(test_h2o, background_frame=background_h2o)\n",
        "        else:\n",
        "            shap_values_h2o = best_model.predict_contributions(test_h2o)\n",
        "        print(\"[SUCCESS] SHAP Values Have Been Calculated.\")\n",
        "        print(\"[SUCCESS] Sub-step 23.1: Calculating SHAP Values Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 23.2: Preparing Plot Data:\n",
        "        print(\"\\n--> Sub-step 23.2: Preparing Plot Data...\")\n",
        "        shap_values_df = shap_values_h2o.as_data_frame(use_multi_thread=True)\n",
        "        shap_values = shap_values_df.iloc[:, :-1].values\n",
        "        stock_vars = [f\"_Daily_Log_Return_{name}\" for name in name_map.values()]\n",
        "        macro_vars = list(fred_map.keys())\n",
        "        key_independent_vars = stock_vars + macro_vars\n",
        "        feature_names = shap_values_df.columns[:-1]\n",
        "        key_indices = [i for i, col in enumerate(feature_names) if col in key_independent_vars]\n",
        "        filtered_shap_values = shap_values[:, key_indices]\n",
        "        original_filtered_names = [feature_names[i] for i in key_indices]\n",
        "        cleaned_feature_names = [name.replace('_Daily_Log_Return_', '').lstrip('_') for name in original_filtered_names]\n",
        "        filtered_test_data = test_df[original_filtered_names].copy()\n",
        "        print(f\"[SUCCESS] Data for {len(cleaned_feature_names)} Key Independent Variables Has Been Prepared for Plotting.\")\n",
        "        print(\"[SUCCESS] Sub-step 23.2: Preparing Plot Data Completed Successfully.\")\n",
        "\n",
        "        #--> Sub-step 23.3: Generating SHAP Summary Plot:\n",
        "        print(\"\\n--> Sub-step 23.3: Generating SHAP Summary Plot...\")\n",
        "        colors = [COLOR_PALETTE['Color4'], COLOR_PALETTE['Color3'], COLOR_PALETTE['Color5']]\n",
        "        cmap = mcolors.LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n",
        "        plt.figure()\n",
        "        shap.summary_plot(filtered_shap_values, filtered_test_data, feature_names=cleaned_feature_names,\n",
        "                          show=False, plot_size=[12, max(8, 0.25 * len(cleaned_feature_names))],\n",
        "                          max_display=len(cleaned_feature_names), cmap=cmap)\n",
        "        plt.title('SHAP Summary Plot for Key Independent Variables')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('SHAP_Summary_Plot.png', dpi=300)\n",
        "        print(\"[SUCCESS] Generated Output File: 'SHAP_Summary_Plot.png'.\")\n",
        "        files.download('SHAP_Summary_Plot.png')\n",
        "        print(\"[SUCCESS] Sub-step 23.3: Generating SHAP Summary Plot Completed Successfully.\")\n",
        "\n",
        "        print(\"\\n[SUCCESS] Step 23: Generating SHAP Summary Plot Completed Successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[ERROR] An Error Occurred During SHAP Summary Plot Generation: {e}\")\n",
        "    finally:\n",
        "        print(\"===========================================================================================\")\n",
        "\n",
        "#=============================================================================\n",
        "#                         --- MAIN EXECUTION FLOW ---\n",
        "#=============================================================================\n",
        "if 'IS_ENVIRONMENT_READY' in locals() and IS_ENVIRONMENT_READY:\n",
        "    try:\n",
        "        def to_persian_numerals(s):\n",
        "            persian_map = str.maketrans('0123456789', '')\n",
        "            return str(s).translate(persian_map)\n",
        "\n",
        "        PROJECT_START_DATE = '1995-09-09'\n",
        "        PROJECT_END_DATE = '2025-09-28'\n",
        "        DEPENDENT_VARIABLE = '_Daily_Log_Return_DJI'\n",
        "\n",
        "        if 'ARE_DATA_ASSETS_READY' in locals() and ARE_DATA_ASSETS_READY:\n",
        "            master_df = create_base_dataframe(PROJECT_START_DATE, PROJECT_END_DATE)\n",
        "            if master_df is not None:\n",
        "                all_ohlcv_data = download_ohlcv_data_batch(ticker_info_df, PROJECT_START_DATE, PROJECT_END_DATE)\n",
        "                if all_ohlcv_data is not None:\n",
        "                    master_df = process_and_merge_stock_features(master_df, all_ohlcv_data, ticker_info_df, SYMBOL_TO_NAME_MAP)\n",
        "                    master_df = fetch_and_merge_fred_data(master_df, FRED_TICKERS_MAP)\n",
        "                    master_df = fetch_and_merge_yfinance_indices(master_df, YFINANCE_TICKERS_MAP)\n",
        "                    master_df = add_qualitative_variables(master_df, EVENTS_DATA_CSV)\n",
        "                    master_df = calculate_dependent_variable(master_df)\n",
        "                    preprocessed_df = handle_missing_values(master_df, ticker_info_df, SYMBOL_TO_NAME_MAP)\n",
        "                    preprocessed_df = handle_outliers(preprocessed_df)\n",
        "                    perform_stationarity_analysis(all_ohlcv_data, preprocessed_df, SYMBOL_TO_NAME_MAP, FRED_TICKERS_MAP, DEPENDENT_VARIABLE); time.sleep(3)\n",
        "                    perform_correlation_analysis(preprocessed_df, DEPENDENT_VARIABLE, SYMBOL_TO_NAME_MAP, FRED_TICKERS_MAP); time.sleep(3)\n",
        "                    optimized_df = optimize_data_types(preprocessed_df)\n",
        "                    final_ordered_df = finalize_data_structure(optimized_df, ticker_info_df, SYMBOL_TO_NAME_MAP, FRED_TICKERS_MAP, YFINANCE_TICKERS_MAP, DEPENDENT_VARIABLE); time.sleep(3)\n",
        "                    generate_descriptive_statistics(final_ordered_df, DEPENDENT_VARIABLE, SYMBOL_TO_NAME_MAP, FRED_TICKERS_MAP); time.sleep(3)\n",
        "                    train_df, val_df, test_df = split_dataset(final_ordered_df)\n",
        "                    if train_df is not None:\n",
        "                        train_scaled_df, val_scaled_df, test_scaled_df = perform_data_standardization(train_df, val_df, test_df, DEPENDENT_VARIABLE)\n",
        "                        if train_scaled_df is not None:\n",
        "                            independent_vars_for_dm_test = [f\"_Daily_Log_Return_{name}\" for name in SYMBOL_TO_NAME_MAP.values()] + list(FRED_TICKERS_MAP.keys())\n",
        "                            independent_vars_for_dm_test = [col for col in independent_vars_for_dm_test if col in train_scaled_df.columns]\n",
        "                            baseline_model_instance = build_baseline_model(train_scaled_df, test_scaled_df, DEPENDENT_VARIABLE, SYMBOL_TO_NAME_MAP, FRED_TICKERS_MAP); time.sleep(3)\n",
        "                            best_h2o_model, h2o_leaderboard = discover_optimal_model_with_h2o(train_scaled_df, val_scaled_df, test_scaled_df, DEPENDENT_VARIABLE)\n",
        "                            if best_h2o_model:\n",
        "                                time.sleep(5)\n",
        "                                perform_statistical_significance_test(baseline_model_instance, best_h2o_model, h2o_leaderboard, train_scaled_df, test_scaled_df, DEPENDENT_VARIABLE, independent_vars_for_dm_test); time.sleep(5)\n",
        "                                generate_feature_importance_report(best_h2o_model, SYMBOL_TO_NAME_MAP, FRED_TICKERS_MAP); time.sleep(3)\n",
        "                                generate_shap_summary_plot(best_h2o_model, train_scaled_df, test_scaled_df, SYMBOL_TO_NAME_MAP, FRED_TICKERS_MAP)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[PIPELINE CRITICAL ERROR] The Execution Halted Due to an Unexpected Error: {e}\")\n",
        "    finally:\n",
        "        if 'h2o' in globals() and h2o.cluster() is not None and h2o.cluster().is_running():\n",
        "            h2o.cluster().shutdown()\n",
        "            print(\"\\n[PIPELINE STATUS] H2O Cluster Has Been Shut Down.\")\n",
        "        print(\"\\n[PIPELINE STATUS] Main Execution Flow Finished.\")"
      ]
    }
  ]
}